{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8uoccF-xdVF"
      },
      "source": [
        "# **ÏÇ¨Ïö©Î≤ï**\n",
        "ÌòÑÏû¨ Ïù¥ ÌååÏùºÏùÄ ÍπÉÌóàÎ∏åÏóêÏÑú Î∞îÎ°ú ÌïÑÏöîÌôîÏùºÏùÑ Íµ¨Í∏Ä ÏΩîÎû©Ïóê Î°úÎìúÌïòÍ≥† Íµ¨Í∏Ä ÎìúÎùºÏù¥Î∏å(ÏûêÎ£å Î∞è Ï†ÄÏû•Í≥µÍ∞Ñ)ÏùÑ Ïù¥Ïö©Ìï¥ÏÑú ÏûëÎèôÌïòÎèÑÎ°ù Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÏùå. Îî∞ÎùºÏÑú Ïù¥Î•º Î∞òÏòÅÌïòÏó¨ ÏïûÎ∂ÄÎ∂ÑÏùÑ Ï°∞Ï†ïÌïòÍ≥† ÌååÏùº Ìå®Ïä§Î•º ÏßÄÏ†ïÌïòÎ©¥ Îê®.\n",
        "\n",
        "Í∑∏Î¶¨Í≥† Í≤∞Í≥ºÍ∞íÏùÑ ÏöîÏïΩÌïòÎäî Optuna Î≥¥Í≥†ÏÑúÎèÑ ÏûêÎèô ÏÉùÏÑ±ÎêòÎãà ÌôïÏù∏ÌïòÏÖîÏÑú ÌååÏùº Í≤ΩÎ°úÎ•º ÌïÑÏöîÏóê Îî∞Îùº ÏßÄÏ†ïÌïòÏÑ∏Ïöî\n",
        "\n",
        "ÏòàÏ∏° ÏïÑÏõÉÌíãÏùÄ predition.csvÌôîÏùºÎ™ÖÏúºÎ°ú Íµ¨Í∏Ä ÎìúÎùºÏù¥Î∏åÏóê ÎÇòÏò§Í≤å ÌòÑÏû¨Îäî ÎêòÏñ¥ ÏûàÏúºÎãà Í≤ΩÎ°ú,Ïù¥Î¶Ñ, Ìè¨Îß∑(ÌòÑÏû¨Îäî 1Í∞ú Î†àÏù¥Î∏î(Ïã†Î¢∞ÎèÑ)Ïù¥ Ï∂îÍ∞Ä ÎêòÏñ¥ÏûàÏùåÎã§!)ÏùÑ ÏàòÏ†ï Î∞òÏòÅÌïòÎ©¥ Îê† ÎìØÌï¥Ïöî. Ïù¥ÌõÑ Î∏îÎü≠Ïóê ÏïÑÏõÉÌíãÏùÑ Î∂ÑÏÑùÌïòÎäî Î≥ÑÎèÑ ÏΩîÎìú Î∏îÎü≠Îì§Ïù¥ ÎêòÏñ¥ ÏûàÏñ¥Ïöî. ÏïÑÏõÉÌíãÏùÑ Î∞îÎ°ú ÌôïÏù∏ÌïòÏãúÍ∏∏!\n",
        "  \n",
        "Ïù¥ÌõÑ Î≥ÑÎèÑ Ï∂îÍ∞Ä Î∏îÎü≠Ïóê ÏïÑÏõÉÌíãÏùÑ ÏºÄÍ∏Ä Ìè¨Îß∑Ïóê ÎßûÏ∂îÏñ¥ ÏûêÎèôÏúºÎ°ú submission ÏöîÍµ¨ÌòïÌÉúÎ°ú Î≥ÄÌôòÌïòÎäî ÌååÏùºÎèÑ Ï§ÄÎπÑÎêòÏñ¥ ÏûàÏúºÎãà Ìå®Ïä§Î•º ÏßÄÏ†ïÌï¥ÏÑú ÌôúÏö©ÌïòÏãúÎ©¥ Îê©ÎãàÎã§."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ÏÜåÍ∞ú Î∞è ÏöîÏïΩ**\n",
        "\n",
        "BERT ModelÏùÄ transformerÎ™®Îç∏Ïóê pre-trained Î™®Îç∏Î°ú boemi-kcbert baseÎ•º ÌôúÏö©. Boemi-kcbertÎäî naverÏùò ÎåìÍ∏ÄÏùÑ Î≤†Ïù¥Ïä§Î°ú Ìï¥ÏÑú ÌïôÏäµÎêú Î™®Îç∏Î°ú ÎπÑÏÜçÏñ¥, Î∂ÄÏ†ïÏñ¥ ÌïúÍ∏Ä Î∂ÑÎ•òÏóêÏÑú Í∞ÄÏû• ÏÑ±Îä•Ïù¥ Ï¢ãÎã§Í≥† ÌèâÍ∞ÄÎê®. Ìï¥Îãπ Î™®Îç∏Î°ú Ïã§Ìñâ Í≤∞Í≥º ÏùºÎ∞ò ÎåÄÌôî Î∂ÑÎ•ò ÏÑ±Í≥ºÍ∞Ä Ï¶ùÍ∞ÄÌïú Í≤ÉÏù¥ Í¥ÄÏ∞∞Îê®. ÌåÄÏùò ÏÑ†ÌÉùÍ∞ÄÎä•Ìïú Î≤†Ïù¥Ïä§ÎùºÏù∏ Î™®Îç∏ Ï§ë ÌïòÎÇòÎùºÍ≥† ÏÉùÍ∞ÅÎê®.\n",
        "\n",
        "ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞Îäî optunaÎ•º ÌôúÏö©, ÏûêÎèôÏµúÏ†ÅÌôîÎ•º ÎèÑÎ™®ÌñàÏùå. optuna Î∞©ÏãùÏùÄ Î®ºÏ†Ä trialÎ°ú ÌïôÏäµÏùÑ ÏßÑÌñâÌï¥ÏÑú Í∑∏Ï§ë ÏµúÏ†Å ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞Î•º ÏÑ†Ï†ïÌï¥ÏÑú ÏµúÏ¢Ö ÌïôÏäµÌïòÎäî Î∞©ÏãùÏúºÎ°ú ÏïΩÍ∞Ñ Îçî Ï∂îÍ∞ÄÏ†ÅÏù∏ ÏãúÍ∞ÑÏÜåÏöîÍ∞Ä Î∞úÏÉùÌï®. Í≤∞Î°†Ï†ÅÏúºÎ°ú ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Î≥ÄÍ≤ΩÎßåÏúºÎ°úÎäî ÎØ∏ÎØ∏Ìïú Í∞úÏÑ†Ìö®Í≥ºÎ•º Î≥¥ÏòÄÍ≥† ÌïµÏã¨Ï†ÅÏù∏ Í∞úÏÑ† ÏòÅÏó≠ÏùÄ ÏïÑÎãàÎùºÍ≥† ÌåêÎã®Îê®.\n",
        "\n",
        "Î≤ÑÌä∏ Î™®Îç∏Ïóê ÎåÄÌïú Ïù¥Ìï¥Í∞Ä Î∂ÄÏ°±Ìï¥ÏÑú ÌÅ¥Î°úÎìúÎ•º ÌÜµÌï¥ÏÑú Ï†ÑÏ≤¥Î•º Íµ¨ÏÑ±ÌïòÍ≥†, geminiÎ•º ÌÜµÌï¥ÏÑú ÏàòÏ†ïÌñàÏùå. Î™®Îç∏ Í∞úÎ∞úÏùÑ Ïö∞ÏÑ†Ï†ÅÏúºÎ°ú ÏôÑÏÑ±ÌïòÍ∏∞ ÏúÑÌïòÏó¨, Î™®ÎìàÌôîÎêú ÌåÄÏùò ÏΩîÎìúÎäî ÏÇ¨Ïö©ÌïòÏßÄ Î™ªÌñàÏúºÎØÄÎ°ú ÍπäÏùÄ ÏñëÌï¥Î•º Î∂ÄÌÉÅÌï©ÎãàÎã§. Ï≤òÏùåÎ∂ÄÌÑ∞ Íµ¨ÏÑ±Ìïú ÏÉÅÌÉúÏûÑ."
      ],
      "metadata": {
        "id": "MwzBwPySig2F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSZu-4gxGIHu",
        "outputId": "7b555e0c-19a0-441e-dda1-1e622ddbeac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GitHub token ÏûÖÎ†•: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Cloning into 'DLthon_pepero_day'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 98 (delta 35), reused 79 (delta 21), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (98/98), 2.81 MiB | 8.52 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n"
          ]
        }
      ],
      "source": [
        "# ÍπÉÌóàÎ∏åÏóêÏÑú ÏΩîÎû©ÏúºÎ°ú ÏûêÎ£åÎ•º Î∞îÎ°ú Í∞ÄÏ†∏Ïò§ÎèÑÎ°ù ÏÖãÌåÖ. Í∞Å ÌôòÍ≤ΩÏóê ÎßûÍ≤å ÏàòÏ†ï ÏöîÎßù\n",
        "from getpass import getpass\n",
        "token = getpass('GitHub token ÏûÖÎ†•: ')\n",
        "!git clone https://{token}@github.com/hiyslee/DLthon_pepero_day.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Íµ¨Í∏Ä ÎìúÎùºÏù¥Î∏åÎ•º Ï†ÄÏû•ÏÜåÎ°ú ÏÑ†ÌÉùÌï®\n",
        "# ============================================================================\n",
        "# 01. Google Drive ÎßàÏö¥Ìä∏\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"01. Google Drive ÎßàÏö¥Ìä∏\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úì Google Drive ÎßàÏö¥Ìä∏ ÏôÑÎ£å\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ6KbOrfYdWU",
        "outputId": "a838e93d-5525-4998-9a3e-7647b1fdbfe1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "01. Google Drive ÎßàÏö¥Ìä∏\n",
            "======================================================================\n",
            "Mounted at /content/drive\n",
            "‚úì Google Drive ÎßàÏö¥Ìä∏ ÏôÑÎ£å\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e972834",
        "outputId": "05593abd-08df-4409-f730-8676ea168743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
            "/bin/bash: line 1: !pip: command not found\n"
          ]
        }
      ],
      "source": [
        "# Ìä∏ÎûúÏä§Ìè¨Î®∏ ÏÑ§Ïπò. ÏóêÎü¨ ÏÉùÍ≤®ÏÑú Í∞úÏÑ†Ìï®\n",
        "!pip install --upgrade pip && !pip install torch && !pip install transformers --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# beomi/kcbert-base Î™®Îç∏ - F1 Score Ï∂îÍ∞Ä Î≤ÑÏ†Ñ\n",
        "\"\"\"\n",
        "ÏßÅÏû• ÎÇ¥ ÎåÄÌôî Î∂ÑÎ•ò - BERT Î™®Îç∏\n",
        "Optuna ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏûêÎèô ÌäúÎãù\n",
        "Google Colab ÌÜµÌï© ÏΩîÎìú\n",
        "\n",
        "Í∏∞Îä•:\n",
        "  ‚Ä¢ OptunaÎ•º Ïù¥Ïö©Ìïú ÏûêÎèô ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù\n",
        "  ‚Ä¢ Í≤ÄÏ¶ù/ÌÖåÏä§Ìä∏ Îã®Í≥ÑÏóê F1 Score Ï∂îÍ∞Ä (Î™®ÎãàÌÑ∞ÎßÅ Î™©Ï†Å)\n",
        "  ‚Ä¢ ÌïôÏäµ/ÏµúÏ†ÅÌôîÎäî Ï†ïÌôïÎèÑ Í∏∞Î∞òÏúºÎ°ú Ïú†ÏßÄ\n",
        "  ‚Ä¢ ÏµúÏ†Å ÌååÎùºÎØ∏ÌÑ∞Î°ú ÏµúÏ¢Ö Î™®Îç∏ ÌïôÏäµ Î∞è ÌèâÍ∞Ä\n",
        "\n",
        "‚ú® Ï£ºÏöî Î≥ÄÍ≤ΩÏÇ¨Ìï≠:\n",
        "  - evaluate_model() Ìï®ÏàòÏóê F1 Score Ï∂îÍ∞Ä\n",
        "  - ÏµúÏ¢Ö Î™®Îç∏ ÌïôÏäµ Î£®ÌîÑÏóê F1 Score Í≥ÑÏÇ∞ Ï∂îÍ∞Ä\n",
        "  - ÌÖåÏä§Ìä∏ Îã®Í≥ÑÏóê F1 Score Í≥ÑÏÇ∞ Ï∂îÍ∞Ä\n",
        "  - Early StoppingÏùÄ Ï†ïÌôïÎèÑ Í∏∞Î∞ò Ïú†ÏßÄ (F1ÏùÄ Î™®ÎãàÌÑ∞ÎßÅÎßå)\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 02. ÌôòÍ≤Ω ÏÑ§Ï†ï - GPU Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî\n",
        "# ============================================================================\n",
        "import os\n",
        "\n",
        "# ‚úÖ PyTorch Î©îÎ™®Î¶¨ Ìï†Îãπ ÏµúÏ†ÅÌôî\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# ‚úÖ Transformers Ï∫êÏãú ÎπÑÌôúÏÑ±Ìôî (ÏÑ†ÌÉùÏÇ¨Ìï≠)\n",
        "os.environ['TRANSFORMERS_OFFLINE'] = '0'\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"02. GPU Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî ÏÑ§Ï†ï ÏôÑÎ£å\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 03. ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"03. ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "packages = ['torch', 'transformers', 'scikit-learn', 'tqdm', 'pandas', 'numpy', 'optuna']\n",
        "for package in packages:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "import time\n",
        "\n",
        "print(\"‚úì Î™®Îì† ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò ÏôÑÎ£å\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 05. Í∏∞Î≥∏ ÏÑ§Ï†ïÍ∞í\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"04. Í∏∞Î≥∏ ÏÑ§Ï†ï\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "OUTPUT_PATH = '/content/drive/My Drive/predictions.csv'\n",
        "TUNING_REPORT_PATH = '/content/drive/My Drive/optuna_tuning_report.txt'\n",
        "# Define TRAIN_PATH and TEST_PATH\n",
        "TRAIN_PATH = '/content/DLthon_pepero_day/Data/aiffel-dl-thon-dktc-online-15/train.csv'\n",
        "TEST_PATH = '/content/DLthon_pepero_day/Data/aiffel-dl-thon-dktc-online-15/test.csv' # Assuming test data is also in this file for now\n",
        "\n",
        "\n",
        "MODEL_NAME = 'beomi/kcbert-base'\n",
        "VALIDATION_RATIO = 0.2\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Optuna ÏÑ§Ï†ï\n",
        "N_TRIALS = 5  # ÌäúÎãù ÏãúÎèÑ ÌöüÏàò (Îçî ÎßéÏùÑÏàòÎ°ù Îçî Ï¢ãÏùÄ Í≤∞Í≥º, ÏãúÍ∞Ñ Ï¶ùÍ∞Ä -> Î≥¥ÏàòÏ†ÅÏúºÎ°ú Ï°∞Ï†ï)\n",
        "TIMEOUT_PER_TRIAL = 1800  # Í∞Å ÏãúÎèÑÎãπ ÏµúÎåÄ ÏãúÍ∞Ñ (Ï¥à)\n",
        "\n",
        "print(f\"‚úì Î™®Îç∏: {MODEL_NAME}\")\n",
        "print(f\"‚úì ÎîîÎ∞îÏù¥Ïä§: {DEVICE}\")\n",
        "print(f\"‚úì Optuna ÏãúÎèÑ ÌöüÏàò: {N_TRIALS}\")\n",
        "print(f\"‚úì Validation ÎπÑÏú®: {VALIDATION_RATIO*100}%\\n\")\n",
        "\n",
        "# ‚úÖ GPU Î©îÎ™®Î¶¨ Ï†ïÎ≥¥ Ï∂úÎ†•\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üíæ GPU Ï†ïÎ≥¥:\")\n",
        "    print(f\"  ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  ‚Ä¢ Ï¥ù Î©îÎ™®Î¶¨: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")\n",
        "    print()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 06. Dataset ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"05. Dataset ÌÅ¥ÎûòÏä§ Ï†ïÏùò\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, conversations, tokenizer, labels=None, max_length=512):\n",
        "        self.conversations = conversations\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.conversations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.conversations[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
        "        }\n",
        "\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "        return item\n",
        "\n",
        "print(\"‚úì Dataset ÌÅ¥ÎûòÏä§ Ï†ïÏùò ÏôÑÎ£å\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 07. ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"06. ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "df_train = pd.read_csv(TRAIN_PATH)\n",
        "print(f\"‚úì ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞: {len(df_train):,}Í∞ú\")\n",
        "\n",
        "df_train = df_train.dropna(subset=['conversation', 'class'])\n",
        "print(f\"‚úì Í≤∞Ï∏°Ïπò Ï†úÍ±∞ ÌõÑ: {len(df_train):,}Í∞ú\")\n",
        "\n",
        "df_train = df_train.drop_duplicates(subset=['conversation', 'class'])\n",
        "print(f\"‚úì Ï§ëÎ≥µ Ï†úÍ±∞ ÌõÑ: {len(df_train):,}Í∞ú\")\n",
        "\n",
        "print(f\"\\nüìä ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨:\")\n",
        "print(df_train['class'].value_counts())\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# 08. ÎùºÎ≤® Ïù∏ÏΩîÎî©\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"07. ÎùºÎ≤® Ïù∏ÏΩîÎî©\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df_train['label'] = label_encoder.fit_transform(df_train['class'])\n",
        "\n",
        "print(f\"‚úì ÌÅ¥ÎûòÏä§ Í∞úÏàò: {len(label_encoder.classes_)}\")\n",
        "print(f\"\\nÌÅ¥ÎûòÏä§ Îß§Ìïë:\")\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    count = (df_train['class'] == class_name).sum()\n",
        "    print(f\"  {i}: {class_name:20s} ({count:,}Í∞ú)\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# 09. ÌïôÏäµ/Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† (90/10)\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"08. ÌïôÏäµ/Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df_train['conversation'].values,\n",
        "    df_train['label'].values,\n",
        "    test_size=VALIDATION_RATIO,\n",
        "    random_state=42,\n",
        "    stratify=df_train['label'].values\n",
        ")\n",
        "\n",
        "print(f\"‚úì ÌïôÏäµ Îç∞Ïù¥ÌÑ∞: {len(X_train):,}Í∞ú ({(1-VALIDATION_RATIO)*100:.0f}%)\")\n",
        "print(f\"‚úì Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞: {len(X_val):,}Í∞ú ({VALIDATION_RATIO*100:.0f}%)\\n\")\n",
        "\n",
        "# ‚úÖ Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù\n",
        "print(f\"\\nüîç Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Í≤ÄÏÇ¨:\")\n",
        "print(f\"  ‚Ä¢ ÌïôÏäµ ÎùºÎ≤® Î≤îÏúÑ: {y_train.min()} ~ {y_train.max()}\")\n",
        "print(f\"  ‚Ä¢ Í≤ÄÏ¶ù ÎùºÎ≤® Î≤îÏúÑ: {y_val.min()} ~ {y_val.max()}\")\n",
        "print(f\"  ‚Ä¢ ÌïôÏäµ ÎùºÎ≤® Ïú†ÎãàÌÅ¨: {np.unique(y_train)}\")\n",
        "print(f\"  ‚Ä¢ Í≤ÄÏ¶ù ÎùºÎ≤® Ïú†ÎãàÌÅ¨: {np.unique(y_val)}\")\n",
        "\n",
        "# ‚úÖ ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ ÏÉòÌîå ÌôïÏù∏\n",
        "print(f\"\\nüìù ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ ÏÉòÌîå (Ï≤òÏùå 3Í∞ú):\")\n",
        "for i in range(min(3, len(X_train))):\n",
        "    print(f\"  {i+1}. Text: {X_train[i][:50]}...\")\n",
        "    print(f\"     Label: {y_train[i]} ({label_encoder.classes_[y_train[i]]})\")\n",
        "\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# 10. ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"09. ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# ‚úÖ Î™®Îç∏Ïùò ÏµúÎåÄ Í∏∏Ïù¥ ÌôïÏù∏\n",
        "from transformers import AutoConfig\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "model_max_length = config.max_position_embeddings\n",
        "\n",
        "print(f\"‚úì ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú ÏôÑÎ£å\")\n",
        "print(f\"‚úì Î™®Îç∏ ÏµúÎåÄ ÏúÑÏπò ÏûÑÎ≤†Îî©: {model_max_length}\")\n",
        "print(f\"‚úì ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÏµúÎåÄ Í∏∏Ïù¥: {min(model_max_length, 512)}\\n\")\n",
        "\n",
        "# ‚úÖ Ïã§Ï†ú ÏÇ¨Ïö© Í∞ÄÎä•Ìïú max_length ÏÑ§Ï†ï\n",
        "SAFE_MAX_LENGTH = min(model_max_length, 512)\n",
        "\n",
        "print(f\"‚úì ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú ÏôÑÎ£å\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 11. Î™®Îç∏ Î°úÎìú Ìï®Ïàò\n",
        "# ============================================================================\n",
        "def create_model(num_labels):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        num_labels=num_labels\n",
        "    ).to(DEVICE)\n",
        "    return model\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"10. Î™®Îç∏ Î°úÎìú Ìï®Ïàò Ï†ïÏùò ÏôÑÎ£å\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 12. Í≤ÄÏ¶ù Ìï®Ïàò (Optuna ÏãúÎèÑÏö©) - ‚úÖ F1 Score Ï∂îÍ∞Ä\n",
        "# ============================================================================\n",
        "def evaluate_model(model, val_loader):\n",
        "    \"\"\"Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Î°ú Î™®Îç∏ ÌèâÍ∞Ä (Accuracy & F1 Score)\"\"\"\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            val_preds.extend(predictions.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(val_labels, val_preds)\n",
        "    # ‚úÖ F1 Score Ï∂îÍ∞Ä (Î™®ÎãàÌÑ∞ÎßÅ Î™©Ï†Å, Early StoppingÏùÄ accuracy Í∏∞Î∞ò)\n",
        "    f1 = f1_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    return accuracy, f1\n",
        "\n",
        "# ============================================================================\n",
        "# 13. Î™©Ï†Å Ìï®Ïàò (OptunaÏö©)\n",
        "# ============================================================================\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"Optuna Î™©Ï†Å Ìï®Ïàò - GPU Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ Í∞ïÌôî\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Trial {trial.number + 1}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # ‚úÖ Trial ÏãúÏûë Ï†Ñ GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n",
        "    torch.cuda.empty_cache()\n",
        "    import gc\n",
        "    gc.collect()\n",
        "\n",
        "    # ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Ï†úÏïà\n",
        "    batch_size = trial.suggest_categorical('batch_size', [8, 16])  # ‚úÖ ÏûëÍ≤å ÏãúÏûë\n",
        "    epochs = trial.suggest_int('epochs', 3, 5)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True)\n",
        "    max_length = trial.suggest_categorical('max_length', [128, 256])  # ‚úÖ ÏûëÍ≤å ÏãúÏûë\n",
        "\n",
        "    print(f\"\\nüìä Ï†úÏïàÎêú ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞:\")\n",
        "    print(f\"  ‚Ä¢ Batch Size: {batch_size}\")\n",
        "    print(f\"  ‚Ä¢ Epochs: {epochs}\")\n",
        "    print(f\"  ‚Ä¢ Learning Rate: {learning_rate:.2e}\")\n",
        "    print(f\"  ‚Ä¢ Max Length: {max_length}\")\n",
        "\n",
        "    # ‚úÖ Ï¥àÍ∏∞ GPU Î©îÎ™®Î¶¨ ÏÉÅÌÉú Ï∂úÎ†•\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"\\nüíæ GPU Î©îÎ™®Î¶¨ (Trial ÏãúÏûë):\")\n",
        "        print(f\"  ‚Ä¢ Ìï†ÎãπÎê®: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
        "        print(f\"  ‚Ä¢ ÏòàÏïΩÎê®: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n",
        "\n",
        "    try:\n",
        "        # Dataset ÏÉùÏÑ±\n",
        "        train_dataset = ConversationDataset(\n",
        "            X_train, tokenizer, labels=y_train, max_length=max_length\n",
        "        )\n",
        "        val_dataset = ConversationDataset(\n",
        "            X_val, tokenizer, labels=y_val, max_length=max_length\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "        # Î™®Îç∏ ÏÉùÏÑ±\n",
        "        model = create_model(len(label_encoder.classes_))\n",
        "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # ÌïôÏäµ\n",
        "        print(f\"\\nüîÑ Î™®Îç∏ ÌïôÏäµ Ï§ë...\")\n",
        "        best_val_accuracy = 0\n",
        "        best_val_f1 = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "            for batch_idx, batch in enumerate(pbar):\n",
        "                input_ids = batch['input_ids'].to(DEVICE)\n",
        "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "                labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                loss = outputs.loss\n",
        "\n",
        "                # ‚úÖ NaN Ï≤¥ÌÅ¨\n",
        "                if torch.isnan(loss):\n",
        "                    print(f\"\\n‚ö†Ô∏è NaN loss Î∞úÍ≤¨! Epoch {epoch+1}, Batch {batch_idx}\")\n",
        "                    raise ValueError(\"Loss became NaN\")\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "\n",
        "                # ‚úÖ Gradient clipping Ï∂îÍ∞Ä\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # ÌïôÏäµ Ï†ïÌôïÎèÑ Ï∂îÏ†Å\n",
        "                predictions = torch.argmax(outputs.logits, dim=1)\n",
        "                train_correct += (predictions == labels).sum().item()\n",
        "                train_total += labels.size(0)\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f'{total_loss/(batch_idx+1):.4f}',\n",
        "                    'acc': f'{train_correct/train_total:.4f}'\n",
        "                })\n",
        "\n",
        "            train_accuracy = train_correct / train_total\n",
        "            avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "            # Í≤ÄÏ¶ù\n",
        "            val_accuracy, val_f1 = evaluate_model(model, val_loader)\n",
        "\n",
        "            print(f\"  Epoch {epoch+1}/{epochs}:\")\n",
        "            print(f\"    Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f}\")\n",
        "            print(f\"    Val Acc: {val_accuracy:.4f} | Val F1: {val_f1:.4f}\")\n",
        "\n",
        "            if val_accuracy > best_val_accuracy:\n",
        "                best_val_accuracy = val_accuracy\n",
        "                best_val_f1 = val_f1\n",
        "\n",
        "            # Ï°∞Í∏∞ Ï¢ÖÎ£å\n",
        "            trial.report(best_val_accuracy, epoch)\n",
        "            if trial.should_prune():\n",
        "                print(f\"  ‚ö†Ô∏è Trial pruned at epoch {epoch+1}\")\n",
        "                raise optuna.TrialPruned()\n",
        "\n",
        "        print(f\"\\n‚úÖ Trial {trial.number + 1} ÏôÑÎ£å\")\n",
        "        print(f\"   Best Val Accuracy: {best_val_accuracy:.4f} ({best_val_accuracy*100:.2f}%)\")\n",
        "        print(f\"   Best Val F1: {best_val_f1:.4f}\")\n",
        "\n",
        "        # ‚úÖ 0.0 Í≤ΩÍ≥†\n",
        "        if best_val_accuracy < 0.01:\n",
        "            print(f\"   üö® Ïã¨Í∞Å: Í≤ÄÏ¶ù Ï†ïÌôïÎèÑÍ∞Ä Í±∞Ïùò 0ÏûÖÎãàÎã§!\")\n",
        "            print(f\"   Îç∞Ïù¥ÌÑ∞ÎÇò ÎùºÎ≤®Ïóê Î¨∏Ï†úÍ∞Ä ÏûàÏùÑ Ïàò ÏûàÏäµÎãàÎã§.\")\n",
        "\n",
        "        return best_val_accuracy\n",
        "\n",
        "    except optuna.TrialPruned:\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Trial {trial.number + 1} Ïã§Ìå®: {str(e)}\")\n",
        "        import traceback\n",
        "        print(f\"ÏÉÅÏÑ∏ ÏóêÎü¨:\\n{traceback.format_exc()}\")\n",
        "        return 0.0\n",
        "\n",
        "    finally:\n",
        "        # ‚úÖ‚úÖ‚úÖ Í∞ÄÏû• Ï§ëÏöî! Trial Ï¢ÖÎ£å ÌõÑ Î©îÎ™®Î¶¨ Ìï¥Ï†ú\n",
        "        print(f\"\\nüßπ GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ï§ë...\")\n",
        "\n",
        "        # Î™®Îç∏Í≥º optimizerÎ•º Î™ÖÏãúÏ†ÅÏúºÎ°ú ÏÇ≠Ï†ú\n",
        "        try:\n",
        "            del model\n",
        "            del optimizer\n",
        "            del train_loader\n",
        "            del val_loader\n",
        "            del train_dataset\n",
        "            del val_dataset\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # GPU Î©îÎ™®Î¶¨ Ìï¥Ï†ú\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"üíæ GPU Î©îÎ™®Î¶¨ (Trial Ï¢ÖÎ£å):\")\n",
        "            print(f\"  ‚Ä¢ Ìï†ÎãπÎê®: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
        "            print(f\"  ‚Ä¢ ÏòàÏïΩÎê®: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"11. Î™©Ï†Å Ìï®Ïàò Ï†ïÏùò ÏôÑÎ£å\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 14. Optuna ÏµúÏ†ÅÌôî Ïã§Ìñâ\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"12. Optuna ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù ÏãúÏûë\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Optuna Study ÏÉùÏÑ±\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "study = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    sampler=sampler,\n",
        "    pruner=optuna.pruners.MedianPruner()\n",
        ")\n",
        "\n",
        "# ÏµúÏ†ÅÌôî Ïã§Ìñâ\n",
        "start_tuning_time = time.time()\n",
        "study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT_PER_TRIAL)\n",
        "tuning_time = time.time() - start_tuning_time\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"‚úÖ Optuna ÌäúÎãù ÏôÑÎ£å!\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 15. ÏµúÏ†Å ÌååÎùºÎØ∏ÌÑ∞ ÌôïÏù∏\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"13. ÏµúÏ†Å ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "best_trial = study.best_trial\n",
        "\n",
        "print(f\"\\nüèÜ ÏµúÍ≥† ÏÑ±Îä• Trial: #{best_trial.number}\")\n",
        "print(f\"   Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: {best_trial.value:.4f} ({best_trial.value*100:.2f}%)\")\n",
        "print(f\"\\nüìä ÏµúÏ†Å ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞:\")\n",
        "for param_name, param_value in best_trial.params.items():\n",
        "    print(f\"  ‚Ä¢ {param_name}: {param_value}\")\n",
        "\n",
        "best_batch_size = best_trial.params['batch_size']\n",
        "best_epochs = best_trial.params['epochs']\n",
        "best_learning_rate = best_trial.params['learning_rate']\n",
        "best_max_length = best_trial.params['max_length']\n",
        "\n",
        "# ============================================================================\n",
        "# 16. Î™®Îì† Trial Í≤∞Í≥º ÏöîÏïΩ\n",
        "# ============================================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"14. Î™®Îì† Trial Í≤∞Í≥º\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "trials_df = study.trials_dataframe()\n",
        "print(f\"Ï¥ù ÏãúÎèÑ ÌöüÏàò: {len(study.trials)}\")\n",
        "print(f\"ÏÑ±Í≥µÌïú Trial: {len([t for t in study.trials if t.state == TrialState.COMPLETE])}\")\n",
        "print(f\"Pruned Trial: {len([t for t in study.trials if t.state == TrialState.PRUNED])}\")\n",
        "\n",
        "print(f\"\\nÏµúÏÉÅÏúÑ 5Í∞ú Trial:\")\n",
        "print(trials_df[['number', 'value', 'params_batch_size', 'params_epochs',\n",
        "                 'params_learning_rate', 'params_max_length']].sort_values('value', ascending=False).head(5).to_string())\n",
        "\n",
        "# ============================================================================\n",
        "# 17. ÏµúÏ†Å ÌååÎùºÎØ∏ÌÑ∞Î°ú ÏµúÏ¢Ö Î™®Îç∏ ÌïôÏäµ\n",
        "# ============================================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"15. ÏµúÏ†Å ÌååÎùºÎØ∏ÌÑ∞Î°ú ÏµúÏ¢Ö Î™®Îç∏ ÌïôÏäµ\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "print(f\"üìù ÌïôÏäµ ÏÑ§Ï†ï:\")\n",
        "print(f\"  ‚Ä¢ Batch Size: {best_batch_size}\")\n",
        "print(f\"  ‚Ä¢ Epochs: {best_epochs}\")\n",
        "print(f\"  ‚Ä¢ Learning Rate: {best_learning_rate:.2e}\")\n",
        "print(f\"  ‚Ä¢ Max Length: {best_max_length}\\n\")\n",
        "\n",
        "# Dataset ÏÉùÏÑ±\n",
        "final_train_dataset = ConversationDataset(\n",
        "    X_train, tokenizer, labels=y_train, max_length=best_max_length\n",
        ")\n",
        "final_val_dataset = ConversationDataset(\n",
        "    X_val, tokenizer, labels=y_val, max_length=best_max_length\n",
        ")\n",
        "\n",
        "final_train_loader = DataLoader(final_train_dataset, batch_size=best_batch_size, shuffle=True)\n",
        "final_val_loader = DataLoader(final_val_dataset, batch_size=best_batch_size)\n",
        "\n",
        "# ÏµúÏ¢Ö Î™®Îç∏ ÏÉùÏÑ±\n",
        "final_model = create_model(len(label_encoder.classes_))\n",
        "final_optimizer = AdamW(final_model.parameters(), lr=best_learning_rate)\n",
        "\n",
        "# ÏµúÏ¢Ö ÌïôÏäµ\n",
        "print(\"üîÑ ÏµúÏ¢Ö Î™®Îç∏ ÌïôÏäµ Ï§ë...\\n\")\n",
        "\n",
        "best_final_accuracy = 0\n",
        "training_history_final = []\n",
        "\n",
        "for epoch in range(best_epochs):\n",
        "    final_model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(final_train_loader, desc=f\"Epoch {epoch+1}/{best_epochs}\")\n",
        "\n",
        "    for batch in pbar:\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "        outputs = final_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        final_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        final_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{total_loss/len(pbar):.4f}',\n",
        "            'acc': f'{correct/total:.4f}'\n",
        "        })\n",
        "\n",
        "    train_avg_loss = total_loss / len(final_train_loader)\n",
        "    train_accuracy = correct / total\n",
        "\n",
        "    # Í≤ÄÏ¶ù (‚úÖ F1 Score Ï∂îÍ∞Ä)\n",
        "    final_model.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    pbar_val = tqdm(final_val_loader, desc=f\"Epoch {epoch+1}/{best_epochs} - Val\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in pbar_val:\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "            outputs = final_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            val_preds.extend(predictions.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "    # ‚úÖ F1 Score Í≥ÑÏÇ∞ Ï∂îÍ∞Ä\n",
        "    val_f1 = f1_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    training_history_final.append({\n",
        "        'epoch': epoch + 1,\n",
        "        'train_loss': train_avg_loss,\n",
        "        'train_acc': train_accuracy,\n",
        "        'val_acc': val_accuracy,\n",
        "        'val_f1': val_f1  # ‚úÖ F1 Score Ï∂îÍ∞Ä\n",
        "    })\n",
        "\n",
        "    print(f\"‚úì Epoch {epoch+1}/{best_epochs} - Train Loss: {train_avg_loss:.4f} | Train Acc: {train_accuracy:.4f} | Val Acc: {val_accuracy:.4f} | Val F1: {val_f1:.4f}\")\n",
        "\n",
        "    if val_accuracy > best_final_accuracy:\n",
        "        best_final_accuracy = val_accuracy\n",
        "\n",
        "print(f\"\\n‚úì ÏµúÏ¢Ö Î™®Îç∏ ÌïôÏäµ ÏôÑÎ£å!\")\n",
        "print(f\"  ÏµúÍ≥† Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: {best_final_accuracy:.4f} ({best_final_accuracy*100:.2f}%)\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 18. ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"16. ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î°úÎìú\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "df_test = pd.read_csv(TEST_PATH)\n",
        "print(f\"‚úì ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞: {len(df_test):,}Í∞ú\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 19. ÌÖåÏä§Ìä∏ Dataset Î∞è DataLoader ÏÉùÏÑ±\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"17. ÌÖåÏä§Ìä∏ Dataset ÏÉùÏÑ±\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "test_dataset = ConversationDataset(\n",
        "    df_test['conversation'].values,\n",
        "    tokenizer,\n",
        "    labels=None,\n",
        "    max_length=best_max_length\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=best_batch_size)\n",
        "\n",
        "print(f\"‚úì ÌÖåÏä§Ìä∏ DataLoader ÏÉùÏÑ± ÏôÑÎ£å\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 20. ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ•ò\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"18. ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ•ò\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "final_model.eval()\n",
        "predictions = []\n",
        "confidences = []\n",
        "\n",
        "print(\"Î∂ÑÎ•ò Ï§ë...\\n\")\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader):\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "\n",
        "        outputs = final_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "        pred_ids = torch.argmax(logits, dim=1)\n",
        "        pred_classes = [label_encoder.classes_[pid] for pid in pred_ids.cpu().numpy()]\n",
        "        conf_scores = torch.max(probs, dim=1)[0].cpu().numpy()\n",
        "\n",
        "        predictions.extend(pred_classes)\n",
        "        confidences.extend(conf_scores)\n",
        "\n",
        "print(\"\\n‚úì Î∂ÑÎ•ò ÏôÑÎ£å!\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 21. Í≤∞Í≥º Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ±\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"19. Í≤∞Í≥º Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ±\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "df_test['prediction'] = predictions\n",
        "df_test['confidence'] = confidences\n",
        "\n",
        "print(f\"‚úì Í≤∞Í≥º Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ± ÏôÑÎ£å\")\n",
        "print(f\"‚úì Ïª¨Îüº: {df_test.columns.tolist()}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 22. Í≤∞Í≥ºÎ•º Google DriveÏóê Ï†ÄÏû•\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"20. Í≤∞Í≥º Ï†ÄÏû• (Google Drive)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "df_test.to_csv(OUTPUT_PATH, index=False, encoding='utf-8')\n",
        "print(f\"‚úì ÏòàÏ∏° Í≤∞Í≥º Ï†ÄÏû•: {OUTPUT_PATH}\")\n",
        "print(f\"‚úì Ï†ÄÏû•Îêú Ìñâ Ïàò: {len(df_test):,}Í∞ú\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 23. Optuna ÌäúÎãù Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Î∞è Ï†ÄÏû•\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"21. Optuna ÌäúÎãù Î≥¥Í≥†ÏÑú ÏÉùÏÑ±\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "report = f\"\"\"\n",
        "{'='*80}\n",
        "ÏßÅÏû• ÎÇ¥ ÎåÄÌôî Î∂ÑÎ•ò Î™®Îç∏ - Optuna ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù Î≥¥Í≥†ÏÑú (F1 Score Ìè¨Ìï®)\n",
        "{'='*80}\n",
        "\n",
        "üìä Í∏∞Î≥∏ ÏÑ§Ï†ï\n",
        "{'='*80}\n",
        "Î™®Îç∏: {MODEL_NAME}\n",
        "Optuna ÏãúÎèÑ ÌöüÏàò: {N_TRIALS}\n",
        "Í∞Å ÏãúÎèÑÎãπ ÏµúÎåÄ ÏãúÍ∞Ñ: {TIMEOUT_PER_TRIAL}Ï¥à\n",
        "Validation ÎπÑÏú®: {VALIDATION_RATIO*100}%\n",
        "ÎîîÎ∞îÏù¥Ïä§: {DEVICE}\n",
        "Sampler: TPE (Tree-structured Parzen Estimator)\n",
        "Pruner: Median\n",
        "\n",
        "‚ú® F1 Score ÌôúÏö©:\n",
        "  ‚Ä¢ Early Stopping: Accuracy Í∏∞Î∞ò (ÌïôÏäµ ÏïàÏ†ïÏÑ±)\n",
        "  ‚Ä¢ Î™®ÎãàÌÑ∞ÎßÅ: F1 Score (ÏÑ±Îä• ÌèâÍ∞Ä Î™©Ï†Å)\n",
        "  ‚Ä¢ Ïù¥Ïú†: ÌïôÏäµ ÏïàÏ†ïÏÑ±Í≥º Í∞ùÍ¥ÄÏÑ± Ïú†ÏßÄ\n",
        "\n",
        "üìÅ Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
        "{'='*80}\n",
        "ÏõêÎ≥∏ ÌïôÏäµ Îç∞Ïù¥ÌÑ∞: {len(df_train):,}Í∞ú\n",
        "‚Üì Î∂ÑÌï† ‚Üì\n",
        "ÌïôÏäµ Îç∞Ïù¥ÌÑ∞: {len(X_train):,}Í∞ú (90%)\n",
        "Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞: {len(X_val):,}Í∞ú (10%)\n",
        "ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞: {len(df_test):,}Í∞ú\n",
        "\n",
        "üè∑Ô∏è  ÌÅ¥ÎûòÏä§ Ï†ïÎ≥¥\n",
        "{'='*80}\n",
        "ÌÅ¥ÎûòÏä§ Í∞úÏàò: {len(label_encoder.classes_)}\n",
        "\n",
        "ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨:\n",
        "\"\"\"\n",
        "\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    count = (df_train['class'] == class_name).sum()\n",
        "    report += f\"  {i}: {class_name:20s} - {count:,}Í∞ú\\n\"\n",
        "\n",
        "report += f\"\"\"\n",
        "{'='*80}\n",
        "‚è±Ô∏è  ÌäúÎãù ÏàòÌñâ ÏãúÍ∞Ñ\n",
        "{'='*80}\n",
        "Ï¥ù ÏÜåÏöî ÏãúÍ∞Ñ: {tuning_time:.1f}Ï¥à ({tuning_time/60:.1f}Î∂Ñ)\n",
        "ÌèâÍ∑† ÏãúÍ∞Ñ/ÏãúÎèÑ: {tuning_time/N_TRIALS:.1f}Ï¥à\n",
        "\n",
        "{'='*80}\n",
        "üìä Optuna ÌäúÎãù Í≤∞Í≥º ÏöîÏïΩ\n",
        "{'='*80}\n",
        "Ï¥ù ÏãúÎèÑ ÌöüÏàò: {len(study.trials)}\n",
        "ÏÑ±Í≥µÌïú Trial: {len([t for t in study.trials if t.state == TrialState.COMPLETE])}\n",
        "Pruned Trial: {len([t for t in study.trials if t.state == TrialState.PRUNED])}\n",
        "\n",
        "üèÜ ÏµúÍ≥† ÏÑ±Îä• Trial\n",
        "{'='*80}\n",
        "Trial Î≤àÌò∏: #{best_trial.number}\n",
        "Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: {best_trial.value:.4f} ({best_trial.value*100:.2f}%)\n",
        "\n",
        "ÏµúÏ†Å ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞:\n",
        "\"\"\"\n",
        "\n",
        "for param_name, param_value in best_trial.params.items():\n",
        "    report += f\"  ‚Ä¢ {param_name}: {param_value}\\n\"\n",
        "\n",
        "report += f\"\"\"\n",
        "{'='*80}\n",
        "Î™®Îì† Trial ÏÑ±Í≥º ÏàúÏúÑ (ÏÉÅÏúÑ 10Í∞ú)\n",
        "{'='*80}\n",
        "\"\"\"\n",
        "\n",
        "sorted_trials = sorted(study.trials, key=lambda t: t.value if t.value is not None else 0, reverse=True)\n",
        "for i, trial in enumerate(sorted_trials[:10], 1):\n",
        "    if trial.value is not None:\n",
        "        report += f\"{i}. Trial #{trial.number}: {trial.value:.4f} ({trial.value*100:.2f}%)\\n\"\n",
        "        report += f\"   Batch Size: {trial.params.get('batch_size')}, \"\n",
        "        report += f\"Epochs: {trial.params.get('epochs')}, \"\n",
        "        report += f\"LR: {trial.params.get('learning_rate'):.2e}, \"\n",
        "        report += f\"Max Length: {trial.params.get('max_length')}\\n\"\n",
        "\n",
        "report += f\"\"\"\n",
        "{'='*80}\n",
        "ÏµúÏ¢Ö Î™®Îç∏ ÌïôÏäµ Í≤∞Í≥º (ÏµúÏ†Å ÌååÎùºÎØ∏ÌÑ∞ ÏÇ¨Ïö©) - ‚úÖ F1 Score Ìè¨Ìï®\n",
        "{'='*80}\n",
        "\n",
        "üìä ÏµúÏ¢Ö Î™®Îç∏ ÌïôÏäµ ÏÑ§Ï†ï\n",
        "Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶à: {best_batch_size}\n",
        "ÏóêÌè¨ÌÅ¨: {best_epochs}\n",
        "ÌïôÏäµÎ•†: {best_learning_rate:.2e}\n",
        "ÏµúÎåÄ Í∏∏Ïù¥: {best_max_length}\n",
        "\n",
        "ÌïôÏäµ Ïù¥Î†• (Accuracy & F1 Score):\n",
        "\"\"\"\n",
        "\n",
        "for hist in training_history_final:\n",
        "    report += f\"Epoch {hist['epoch']}: \"\n",
        "    report += f\"Train Loss={hist['train_loss']:.4f} | \"\n",
        "    report += f\"Train Acc={hist['train_acc']:.4f} | \"\n",
        "    report += f\"Val Acc={hist['val_acc']:.4f} | \"\n",
        "    report += f\"Val F1={hist['val_f1']:.4f}\\n\"\n",
        "\n",
        "max_val_f1 = max([h['val_f1'] for h in training_history_final])\n",
        "\n",
        "report += f\"\"\"\n",
        "ÏµúÍ≥† Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: {best_final_accuracy:.4f} ({best_final_accuracy*100:.2f}%)\n",
        "ÏµúÍ≥† Í≤ÄÏ¶ù F1 Score: {max_val_f1:.4f} ({max_val_f1*100:.2f}%)\n",
        "\n",
        "üéØ ÌÖåÏä§Ìä∏ ÏÑ∏Ìä∏ Î∂ÑÎ•ò Í≤∞Í≥º - ‚úÖ F1 Score Ìè¨Ìï®\n",
        "{'='*80}\n",
        "Ï¥ù ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞: {len(df_test):,}Í∞ú\n",
        "\n",
        "ÌÅ¥ÎûòÏä§Î≥Ñ Î∂ÑÌè¨:\n",
        "\"\"\"\n",
        "\n",
        "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌïú F1 Score Í≥ÑÏÇ∞ (Ïã§Ï†ú ÎùºÎ≤®Ïù¥ ÏûàÎäî Í≤ΩÏö∞ ÌïÑÏöî)\n",
        "# Ïó¨Í∏∞ÏÑúÎäî Î∂ÑÎ•ò Í≤∞Í≥ºÎßå ÌëúÏãú\n",
        "for pred_class in label_encoder.classes_:\n",
        "    count = (df_test['prediction'] == pred_class).sum()\n",
        "    percentage = (count / len(df_test)) * 100\n",
        "    report += f\"  {pred_class:20s}: {count:3d}Í∞ú ({percentage:5.1f}%)\\n\"\n",
        "\n",
        "report += f\"\"\"\n",
        "Ïã†Î¢∞ÎèÑ ÌÜµÍ≥Ñ:\n",
        "  ÌèâÍ∑†: {df_test['confidence'].mean():.4f}\n",
        "  ÏµúÍ≥†: {df_test['confidence'].max():.4f}\n",
        "  ÏµúÏ†Ä: {df_test['confidence'].min():.4f}\n",
        "  ÌëúÏ§ÄÌé∏Ï∞®: {df_test['confidence'].std():.4f}\n",
        "\n",
        "Ïã†Î¢∞ÎèÑ Îì±Í∏â:\n",
        "\"\"\"\n",
        "\n",
        "high_conf = (df_test['confidence'] >= 0.9).sum()\n",
        "mid_conf = ((df_test['confidence'] >= 0.7) & (df_test['confidence'] < 0.9)).sum()\n",
        "low_conf = (df_test['confidence'] < 0.7).sum()\n",
        "\n",
        "report += f\"  ÎÜíÏùå (‚â•0.9):  {high_conf:3d}Í∞ú ({high_conf/len(df_test)*100:5.1f}%)\\n\"\n",
        "report += f\"  Ï§ëÍ∞Ñ (0.7~0.9): {mid_conf:3d}Í∞ú ({mid_conf/len(df_test)*100:5.1f}%)\\n\"\n",
        "report += f\"  ÎÇÆÏùå (<0.7):  {low_conf:3d}Í∞ú ({low_conf/len(df_test)*100:5.1f}%)\\n\"\n",
        "\n",
        "report += f\"\"\"\n",
        "{'='*80}\n",
        "üìù F1 Score ÏÇ¨Ïö© Î∞©Ïãù\n",
        "{'='*80}\n",
        "\n",
        "1Ô∏è‚É£  Optuna ÌäúÎãù Îã®Í≥Ñ:\n",
        "   ‚Ä¢ ÏµúÏ†ÅÌôî ÏßÄÌëú: Accuracy (Early Stopping Î∞è Trial Pruning Í∏∞Ï§Ä)\n",
        "   ‚Ä¢ Î™®ÎãàÌÑ∞ÎßÅ: F1 Score (ÏÑ±Îä• ÌèâÍ∞Ä Î™©Ï†Å)\n",
        "   ‚Ä¢ Ïù¥Ïú†: ÌïôÏäµ ÏïàÏ†ïÏÑ±Í≥º Í∞ùÍ¥ÄÏÑ± Ïú†ÏßÄ\n",
        "\n",
        "2Ô∏è‚É£  ÏµúÏ¢Ö Î™®Îç∏ ÌèâÍ∞Ä:\n",
        "   ‚Ä¢ Í≤ÄÏ¶ù ÏÑ∏Ìä∏: Accuracy + F1 Score Î™®Îëê ÌëúÏãú\n",
        "   ‚Ä¢ ÌÖåÏä§Ìä∏ ÏÑ∏Ìä∏: Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò Î∂ÑÎ•ò + F1 Ï†ïÎ≥¥ Ï†úÍ≥µ\n",
        "   ‚Ä¢ ÏùòÎØ∏: Î™®Îç∏Ïùò Îã§ÏñëÌïú ÏÑ±Îä• ÏßÄÌëú ÌôïÏù∏\n",
        "\n",
        "{'='*80}\n",
        "üìù Í≤∞Î°† Î∞è Í∂åÏû•ÏÇ¨Ìï≠\n",
        "{'='*80}\n",
        "\n",
        "1. ÏµúÏ†Å ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞:\n",
        "   ‚Ä¢ Batch Size: {best_batch_size}\n",
        "   ‚Ä¢ Epochs: {best_epochs}\n",
        "   ‚Ä¢ Learning Rate: {best_learning_rate:.2e}\n",
        "   ‚Ä¢ Max Length: {best_max_length}\n",
        "\n",
        "2. Î™®Îç∏ ÏÑ±Îä•:\n",
        "   ‚Ä¢ Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: {best_final_accuracy:.4f}\n",
        "   ‚Ä¢ Í≤ÄÏ¶ù F1 Score: {max_val_f1:.4f}\n",
        "   ‚Ä¢ ÌÖåÏä§Ìä∏ ÏÑ∏Ìä∏ÏóêÏÑú {high_conf}Í∞ú ({high_conf/len(df_test)*100:.1f}%)Ïùò ÎÜíÏùÄ Ïã†Î¢∞ÎèÑ ÏòàÏ∏°\n",
        "\n",
        "3. Îã§Ïùå Í∞úÏÑ† Î∞©Ìñ•:\n",
        "   ‚Ä¢ Îçî ÎßéÏùÄ Trial ÏàòÌñâ (ÌòÑÏû¨ {N_TRIALS})\n",
        "   ‚Ä¢ Îã§Î•∏ Î™®Îç∏ ÏãúÎèÑ (Ïòà: klue/roberta-base)\n",
        "   ‚Ä¢ Ï∂îÍ∞Ä Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï\n",
        "   ‚Ä¢ ÏïôÏÉÅÎ∏î Î™®Îç∏ Ï†ÅÏö©\n",
        "   ‚Ä¢ Class Imbalance Ï≤òÎ¶¨ (weighted loss)\n",
        "\n",
        "{'='*80}\n",
        "ÏÉùÏÑ± ÎÇ†Ïßú: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "{'='*80}\n",
        "\"\"\"\n",
        "\n",
        "with open(TUNING_REPORT_PATH, 'w', encoding='utf-8') as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"‚úì Optuna ÌäúÎãù Î≥¥Í≥†ÏÑú Ï†ÄÏû•: {TUNING_REPORT_PATH}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 24. ÏµúÏ¢Ö ÏôÑÎ£å Î©îÏãúÏßÄ\n",
        "# ============================================================================\n",
        "print(\"\\n\\n\" + \"‚ñà\"*70)\n",
        "print(\"‚ñà\" + \" \"*68 + \"‚ñà\")\n",
        "print(\"‚ñà\" + \"  ‚úÖ Optuna ÏûêÎèô ÌäúÎãù Î∞è Î™®Îì† ÏûëÏóÖÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§!\".center(68) + \"‚ñà\")\n",
        "print(\"‚ñà\" + \" \"*68 + \"‚ñà\")\n",
        "print(\"‚ñà\"*70)\n",
        "\n",
        "print(\"\\nüìÅ Ï†ÄÏû•Îêú ÌååÏùº Ï†ïÎ≥¥:\")\n",
        "print(f\"\\n  üìä ÌååÏùº 1: predictions.csv\")\n",
        "print(f\"     ÏúÑÏπò: My Drive/predictions.csv\")\n",
        "print(f\"     ÎÇ¥Ïö©: ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ•ò Í≤∞Í≥º\")\n",
        "print(f\"     Ìñâ Ïàò: {len(df_test):,}Í∞ú\")\n",
        "\n",
        "print(f\"\\n  üìã ÌååÏùº 2: optuna_tuning_report.txt\")\n",
        "print(f\"     ÏúÑÏπò: My Drive/optuna_tuning_report.txt\")\n",
        "print(f\"     ÎÇ¥Ïö©: Optuna ÌäúÎãù ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑú (F1 Score Ìè¨Ìï®)\")\n",
        "\n",
        "print(f\"\\nüéØ ÏµúÏ†Å ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ (Optuna ÏÑ†Ï†ï):\")\n",
        "print(f\"  ‚Ä¢ Batch Size: {best_batch_size}\")\n",
        "print(f\"  ‚Ä¢ Epochs: {best_epochs}\")\n",
        "print(f\"  ‚Ä¢ Learning Rate: {best_learning_rate:.2e}\")\n",
        "print(f\"  ‚Ä¢ Max Length: {best_max_length}\")\n",
        "\n",
        "print(f\"\\nüìà ÏµúÏ¢Ö Î™®Îç∏ ÏÑ±Îä•:\")\n",
        "print(f\"  ‚Ä¢ Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: {best_final_accuracy:.4f} ({best_final_accuracy*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Í≤ÄÏ¶ù F1 Score: {max_val_f1:.4f} ({max_val_f1*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ ÎÜíÏùÄ Ïã†Î¢∞ÎèÑ ÏòàÏ∏°: {high_conf}Í∞ú ({high_conf/len(df_test)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è  ÏàòÌñâ ÏãúÍ∞Ñ:\")\n",
        "print(f\"  ‚Ä¢ Optuna ÌäúÎãù: {tuning_time:.1f}Ï¥à ({tuning_time/60:.1f}Î∂Ñ)\")\n",
        "print(f\"  ‚Ä¢ Ï¥ù {N_TRIALS}Î≤à ÏãúÎèÑ\")\n",
        "\n",
        "print(f\"\\n‚ú® F1 Score ÌôúÏö©:\")\n",
        "print(f\"  ‚Ä¢ Í≤ÄÏ¶ù Îã®Í≥Ñ: Accuracy Í∏∞Î∞ò ÏµúÏ†ÅÌôî (F1 ScoreÎäî Î™®ÎãàÌÑ∞ÎßÅ)\")\n",
        "print(f\"  ‚Ä¢ ÌÖåÏä§Ìä∏ Îã®Í≥Ñ: Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò Î∂ÑÎ•ò + F1 Ï†ïÎ≥¥ Ï†úÍ≥µ\")\n",
        "print(f\"  ‚Ä¢ ÌïôÏäµÏóê ÏòÅÌñ• ÏóÜÏùå: ÏµúÏ†ÅÌôî Í∏∞Ï§ÄÏùÄ Accuracy Ïú†ÏßÄ\")\n",
        "\n",
        "print(f\"\\nüíæ Google DriveÏóêÏÑú ÌååÏùºÏùÑ Îã§Ïö¥Î°úÎìúÌïòÍ±∞ÎÇò ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§!\")\n",
        "print(\"\\n\" + \"‚ñà\"*70 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWCFFwQDfYH9",
        "outputId": "37004df3-b827-441e-d019-8a6b191a15e0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "02. GPU Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî ÏÑ§Ï†ï ÏôÑÎ£å\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "03. ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò\n",
            "======================================================================\n",
            "‚úì Î™®Îì† ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò ÏôÑÎ£å\n",
            "\n",
            "======================================================================\n",
            "04. Í∏∞Î≥∏ ÏÑ§Ï†ï\n",
            "======================================================================\n",
            "‚úì Î™®Îç∏: beomi/kcbert-base\n",
            "‚úì ÎîîÎ∞îÏù¥Ïä§: cuda\n",
            "‚úì Optuna ÏãúÎèÑ ÌöüÏàò: 5\n",
            "‚úì Validation ÎπÑÏú®: 20.0%\n",
            "\n",
            "üíæ GPU Ï†ïÎ≥¥:\n",
            "  ‚Ä¢ GPU: Tesla T4\n",
            "  ‚Ä¢ Ï¥ù Î©îÎ™®Î¶¨: 14.74 GB\n",
            "\n",
            "======================================================================\n",
            "05. Dataset ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
            "======================================================================\n",
            "‚úì Dataset ÌÅ¥ÎûòÏä§ Ï†ïÏùò ÏôÑÎ£å\n",
            "\n",
            "======================================================================\n",
            "06. ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨\n",
            "======================================================================\n",
            "‚úì ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞: 4,950Í∞ú\n",
            "‚úì Í≤∞Ï∏°Ïπò Ï†úÍ±∞ ÌõÑ: 4,950Í∞ú\n",
            "‚úì Ï§ëÎ≥µ Ï†úÍ±∞ ÌõÑ: 4,619Í∞ú\n",
            "\n",
            "üìä ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨:\n",
            "class\n",
            "Í∏∞ÌÉÄ Í¥¥Î°≠Ìûò ÎåÄÌôî      1011\n",
            "Í∞àÏ∑® ÎåÄÌôî           973\n",
            "ÏßÅÏû• ÎÇ¥ Í¥¥Î°≠Ìûò ÎåÄÌôî     970\n",
            "ÌòëÎ∞ï ÎåÄÌôî           892\n",
            "ÏùºÎ∞ò ÎåÄÌôî           773\n",
            "Name: count, dtype: int64\n",
            "\n",
            "======================================================================\n",
            "07. ÎùºÎ≤® Ïù∏ÏΩîÎî©\n",
            "======================================================================\n",
            "‚úì ÌÅ¥ÎûòÏä§ Í∞úÏàò: 5\n",
            "\n",
            "ÌÅ¥ÎûòÏä§ Îß§Ìïë:\n",
            "  0: Í∞àÏ∑® ÎåÄÌôî                (973Í∞ú)\n",
            "  1: Í∏∞ÌÉÄ Í¥¥Î°≠Ìûò ÎåÄÌôî            (1,011Í∞ú)\n",
            "  2: ÏùºÎ∞ò ÎåÄÌôî                (773Í∞ú)\n",
            "  3: ÏßÅÏû• ÎÇ¥ Í¥¥Î°≠Ìûò ÎåÄÌôî          (970Í∞ú)\n",
            "  4: ÌòëÎ∞ï ÎåÄÌôî                (892Í∞ú)\n",
            "\n",
            "======================================================================\n",
            "08. ÌïôÏäµ/Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
            "======================================================================\n",
            "‚úì ÌïôÏäµ Îç∞Ïù¥ÌÑ∞: 3,695Í∞ú (80%)\n",
            "‚úì Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞: 924Í∞ú (20%)\n",
            "\n",
            "\n",
            "üîç Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Í≤ÄÏÇ¨:\n",
            "  ‚Ä¢ ÌïôÏäµ ÎùºÎ≤® Î≤îÏúÑ: 0 ~ 4\n",
            "  ‚Ä¢ Í≤ÄÏ¶ù ÎùºÎ≤® Î≤îÏúÑ: 0 ~ 4\n",
            "  ‚Ä¢ ÌïôÏäµ ÎùºÎ≤® Ïú†ÎãàÌÅ¨: [0 1 2 3 4]\n",
            "  ‚Ä¢ Í≤ÄÏ¶ù ÎùºÎ≤® Ïú†ÎãàÌÅ¨: [0 1 2 3 4]\n",
            "\n",
            "üìù ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ ÏÉòÌîå (Ï≤òÏùå 3Í∞ú):\n",
            "  1. Text: Ìï†Î®∏Îãà ÎèàÏ¢Ä ÎπåÎ†§Ï£ºÏÑ∏Ïöî\n",
            "Ïôú ?\n",
            "Îã¥Î∞∞ ÏÇ¨Í≤åÏöî\n",
            "Ïù¥Í≤ÉÎì§Ïù¥ ÎØ∏Ï≤¨ÎÇò?\n",
            "Ìï†Î®∏Îãà Îèà ÎßéÏûñÏïÑ\n",
            "Ïù¥Î¶¨Îì§ÏôÄ Ìòº...\n",
            "     Label: 0 (Í∞àÏ∑® ÎåÄÌôî)\n",
            "  2. Text: Î∞• Î®πÏóàÏñ¥? Ï†êÏã¨ÏãúÍ∞ÑÏóê Î≠ê ÌñàÏñ¥?Ïïà Ïã¨Ïã¨Ìï¥?\n",
            "ÎÑ§.\n",
            "Îòê Îã®ÎãµÏù¥ÎÑ§\n",
            "ÏïÑÏãúÏ£†?\n",
            "Î≠ò ÏïåÏïÑ?\n",
            "Ïù¥ÏÉÅÌïú ...\n",
            "     Label: 3 (ÏßÅÏû• ÎÇ¥ Í¥¥Î°≠Ìûò ÎåÄÌôî)\n",
            "  3. Text: Ïó¨Í∏∞ Ïó¨Í∏∞!!!Ïù¥Î¥ê!!!\n",
            "ÎÑ§ÏÜêÎãò Î¨¥Ïä®ÏùºÏù¥ÏãúÏ£†?\n",
            "Ïù¥Í±∞ Î≥¥Ïó¨? Ïù¥ Î®∏Î¶¨Ïπ¥ÎùΩ Î≠êÎÉêÍ≥†!!!!\n",
            "ÏïÑÏ£Ñ...\n",
            "     Label: 0 (Í∞àÏ∑® ÎåÄÌôî)\n",
            "\n",
            "======================================================================\n",
            "09. ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-09 03:04:32,861] A new study created in memory with name: no-name-c19cf96a-3739-466e-98c2-d2d9408482c7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú ÏôÑÎ£å\n",
            "‚úì Î™®Îç∏ ÏµúÎåÄ ÏúÑÏπò ÏûÑÎ≤†Îî©: 300\n",
            "‚úì ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÏµúÎåÄ Í∏∏Ïù¥: 300\n",
            "\n",
            "‚úì ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú ÏôÑÎ£å\n",
            "\n",
            "======================================================================\n",
            "10. Î™®Îç∏ Î°úÎìú Ìï®Ïàò Ï†ïÏùò ÏôÑÎ£å\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "11. Î™©Ï†Å Ìï®Ïàò Ï†ïÏùò ÏôÑÎ£å\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "12. Optuna ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù ÏãúÏûë\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Trial 1\n",
            "======================================================================\n",
            "\n",
            "üìä Ï†úÏïàÎêú ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞:\n",
            "  ‚Ä¢ Batch Size: 16\n",
            "  ‚Ä¢ Epochs: 5\n",
            "  ‚Ä¢ Learning Rate: 2.62e-05\n",
            "  ‚Ä¢ Max Length: 128\n",
            "\n",
            "üíæ GPU Î©îÎ™®Î¶¨ (Trial ÏãúÏûë):\n",
            "  ‚Ä¢ Ìï†ÎãπÎê®: 2.47 GB\n",
            "  ‚Ä¢ ÏòàÏïΩÎê®: 7.81 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÑ Î™®Îç∏ ÌïôÏäµ Ï§ë...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1/5:\n",
            "    Train Loss: 0.5213 | Train Acc: 0.8152\n",
            "    Val Acc: 0.9004 | Val F1: 0.9007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2/5:\n",
            "    Train Loss: 0.1948 | Train Acc: 0.9415\n",
            "    Val Acc: 0.8961 | Val F1: 0.8960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3/5:\n",
            "    Train Loss: 0.1012 | Train Acc: 0.9727\n",
            "    Val Acc: 0.8896 | Val F1: 0.8903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4/5:\n",
            "    Train Loss: 0.0398 | Train Acc: 0.9892\n",
            "    Val Acc: 0.8842 | Val F1: 0.8849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 5/5:\n",
            "    Train Loss: 0.0120 | Train Acc: 0.9970\n",
            "    Val Acc: 0.8994 | Val F1: 0.8988\n",
            "\n",
            "‚úÖ Trial 1 ÏôÑÎ£å\n",
            "   Best Val Accuracy: 0.9004 (90.04%)\n",
            "   Best Val F1: 0.9007\n",
            "\n",
            "üßπ GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ï§ë...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-09 03:11:58,807] Trial 0 finished with value: 0.9004329004329005 and parameters: {'batch_size': 16, 'epochs': 5, 'learning_rate': 2.620863021537753e-05, 'max_length': 128}. Best is trial 0 with value: 0.9004329004329005.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ GPU Î©îÎ™®Î¶¨ (Trial Ï¢ÖÎ£å):\n",
            "  ‚Ä¢ Ìï†ÎãπÎê®: 3.30 GB\n",
            "  ‚Ä¢ ÏòàÏïΩÎê®: 6.10 GB\n",
            "\n",
            "======================================================================\n",
            "Trial 2\n",
            "======================================================================\n",
            "\n",
            "üìä Ï†úÏïàÎêú ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞:\n",
            "  ‚Ä¢ Batch Size: 16\n",
            "  ‚Ä¢ Epochs: 4\n",
            "  ‚Ä¢ Learning Rate: 3.13e-05\n",
            "  ‚Ä¢ Max Length: 256\n",
            "\n",
            "üíæ GPU Î©îÎ™®Î¶¨ (Trial ÏãúÏûë):\n",
            "  ‚Ä¢ Ìï†ÎãπÎê®: 2.47 GB\n",
            "  ‚Ä¢ ÏòàÏïΩÎê®: 6.10 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÑ Î™®Îç∏ ÌïôÏäµ Ï§ë...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1/4:\n",
            "    Train Loss: 0.4571 | Train Acc: 0.8417\n",
            "    Val Acc: 0.8874 | Val F1: 0.8871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2/4:\n",
            "    Train Loss: 0.1739 | Train Acc: 0.9505\n",
            "    Val Acc: 0.9015 | Val F1: 0.9014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3/4:\n",
            "    Train Loss: 0.0893 | Train Acc: 0.9770\n",
            "    Val Acc: 0.8983 | Val F1: 0.8988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4/4:\n",
            "    Train Loss: 0.0577 | Train Acc: 0.9876\n",
            "    Val Acc: 0.9102 | Val F1: 0.9096\n",
            "\n",
            "‚úÖ Trial 2 ÏôÑÎ£å\n",
            "   Best Val Accuracy: 0.9102 (91.02%)\n",
            "   Best Val F1: 0.9096\n",
            "\n",
            "üßπ GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ï§ë...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-09 03:23:11,524] Trial 1 finished with value: 0.9101731601731602 and parameters: {'batch_size': 16, 'epochs': 4, 'learning_rate': 3.12551431816761e-05, 'max_length': 256}. Best is trial 1 with value: 0.9101731601731602.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ GPU Î©îÎ™®Î¶¨ (Trial Ï¢ÖÎ£å):\n",
            "  ‚Ä¢ Ìï†ÎãπÎê®: 3.31 GB\n",
            "  ‚Ä¢ ÏòàÏïΩÎê®: 6.40 GB\n",
            "\n",
            "======================================================================\n",
            "Trial 3\n",
            "======================================================================\n",
            "\n",
            "üìä Ï†úÏïàÎêú ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞:\n",
            "  ‚Ä¢ Batch Size: 8\n",
            "  ‚Ä¢ Epochs: 3\n",
            "  ‚Ä¢ Learning Rate: 1.34e-05\n",
            "  ‚Ä¢ Max Length: 256\n",
            "\n",
            "üíæ GPU Î©îÎ™®Î¶¨ (Trial ÏãúÏûë):\n",
            "  ‚Ä¢ Ìï†ÎãπÎê®: 2.47 GB\n",
            "  ‚Ä¢ ÏòàÏïΩÎê®: 6.10 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÑ Î™®Îç∏ ÌïôÏäµ Ï§ë...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1/3:\n",
            "    Train Loss: 0.4992 | Train Acc: 0.8349\n",
            "    Val Acc: 0.9015 | Val F1: 0.9009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2/3:\n",
            "    Train Loss: 0.2041 | Train Acc: 0.9459\n",
            "    Val Acc: 0.9026 | Val F1: 0.9025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3/3:\n",
            "    Train Loss: 0.1023 | Train Acc: 0.9767\n",
            "    Val Acc: 0.9080 | Val F1: 0.9074\n",
            "\n",
            "‚úÖ Trial 3 ÏôÑÎ£å\n",
            "   Best Val Accuracy: 0.9080 (90.80%)\n",
            "   Best Val F1: 0.9074\n",
            "\n",
            "üßπ GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ï§ë...\n",
            "üíæ GPU Î©îÎ™®Î¶¨ (Trial Ï¢ÖÎ£å):\n",
            "  ‚Ä¢ Ìï†ÎãπÎê®: 3.30 GB\n",
            "  ‚Ä¢ ÏòàÏïΩÎê®: 6.10 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-09 03:32:20,217] Trial 2 finished with value: 0.908008658008658 and parameters: {'batch_size': 8, 'epochs': 3, 'learning_rate': 1.34336568680343e-05, 'max_length': 256}. Best is trial 1 with value: 0.9101731601731602.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Trial 4\n",
            "======================================================================\n",
            "\n",
            "üìä Ï†úÏïàÎêú ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞:\n",
            "  ‚Ä¢ Batch Size: 8\n",
            "  ‚Ä¢ Epochs: 4\n",
            "  ‚Ä¢ Learning Rate: 1.25e-05\n",
            "  ‚Ä¢ Max Length: 256\n",
            "\n",
            "üíæ GPU Î©îÎ™®Î¶¨ (Trial ÏãúÏûë):\n",
            "  ‚Ä¢ Ìï†ÎãπÎê®: 2.47 GB\n",
            "  ‚Ä¢ ÏòàÏïΩÎê®: 6.10 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÑ Î™®Îç∏ ÌïôÏäµ Ï§ë...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1/4:\n",
            "    Train Loss: 0.4993 | Train Acc: 0.8363\n",
            "    Val Acc: 0.9058 | Val F1: 0.9055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2/4:\n",
            "    Train Loss: 0.2182 | Train Acc: 0.9415\n",
            "    Val Acc: 0.9134 | Val F1: 0.9132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3/4:\n",
            "    Train Loss: 0.1103 | Train Acc: 0.9737\n",
            "    Val Acc: 0.9037 | Val F1: 0.9040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4/4:\n",
            "    Train Loss: 0.0602 | Train Acc: 0.9876\n",
            "    Val Acc: 0.9091 | Val F1: 0.9091\n",
            "\n",
            "‚úÖ Trial 4 ÏôÑÎ£å\n",
            "   Best Val Accuracy: 0.9134 (91.34%)\n",
            "   Best Val F1: 0.9132\n",
            "\n",
            "üßπ GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ï§ë...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-09 03:44:31,795] Trial 3 finished with value: 0.9134199134199135 and parameters: {'batch_size': 8, 'epochs': 4, 'learning_rate': 1.251705107614021e-05, 'max_length': 256}. Best is trial 3 with value: 0.9134199134199135.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ GPU Î©îÎ™®Î¶¨ (Trial Ï¢ÖÎ£å):\n",
            "  ‚Ä¢ Ìï†ÎãπÎê®: 3.30 GB\n",
            "  ‚Ä¢ ÏòàÏïΩÎê®: 6.10 GB\n",
            "\n",
            "======================================================================\n",
            "‚úÖ Optuna ÌäúÎãù ÏôÑÎ£å!\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "13. ÏµúÏ†Å ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞\n",
            "======================================================================\n",
            "\n",
            "üèÜ ÏµúÍ≥† ÏÑ±Îä• Trial: #3\n",
            "   Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 0.9134 (91.34%)\n",
            "\n",
            "üìä ÏµúÏ†Å ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞:\n",
            "  ‚Ä¢ batch_size: 8\n",
            "  ‚Ä¢ epochs: 4\n",
            "  ‚Ä¢ learning_rate: 1.251705107614021e-05\n",
            "  ‚Ä¢ max_length: 256\n",
            "\n",
            "======================================================================\n",
            "14. Î™®Îì† Trial Í≤∞Í≥º\n",
            "======================================================================\n",
            "\n",
            "Ï¥ù ÏãúÎèÑ ÌöüÏàò: 4\n",
            "ÏÑ±Í≥µÌïú Trial: 4\n",
            "Pruned Trial: 0\n",
            "\n",
            "ÏµúÏÉÅÏúÑ 5Í∞ú Trial:\n",
            "   number     value  params_batch_size  params_epochs  params_learning_rate  params_max_length\n",
            "3       3  0.913420                  8              4              0.000013                256\n",
            "1       1  0.910173                 16              4              0.000031                256\n",
            "2       2  0.908009                  8              3              0.000013                256\n",
            "0       0  0.900433                 16              5              0.000026                128\n",
            "\n",
            "======================================================================\n",
            "15. ÏµúÏ†Å ÌååÎùºÎØ∏ÌÑ∞Î°ú ÏµúÏ¢Ö Î™®Îç∏ ÌïôÏäµ\n",
            "======================================================================\n",
            "\n",
            "üìù ÌïôÏäµ ÏÑ§Ï†ï:\n",
            "  ‚Ä¢ Batch Size: 8\n",
            "  ‚Ä¢ Epochs: 4\n",
            "  ‚Ä¢ Learning Rate: 1.25e-05\n",
            "  ‚Ä¢ Max Length: 256\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ ÏµúÏ¢Ö Î™®Îç∏ ÌïôÏäµ Ï§ë...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 462/462 [02:47<00:00,  2.77it/s, loss=0.5244, acc=0.8179]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Epoch 1/4 - Train Loss: 0.5244 | Train Acc: 0.8179 | Val Acc: 0.8939 | Val F1: 0.8925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 462/462 [02:47<00:00,  2.77it/s, loss=0.1583, acc=0.9494]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Epoch 2/4 - Train Loss: 0.1583 | Train Acc: 0.9494 | Val Acc: 0.9091 | Val F1: 0.9084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 462/462 [02:46<00:00,  2.77it/s, loss=0.0691, acc=0.9827]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Epoch 3/4 - Train Loss: 0.0691 | Train Acc: 0.9827 | Val Acc: 0.9058 | Val F1: 0.9060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 462/462 [02:46<00:00,  2.77it/s, loss=0.0375, acc=0.9886]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Epoch 4/4 - Train Loss: 0.0375 | Train Acc: 0.9886 | Val Acc: 0.9048 | Val F1: 0.9046\n",
            "\n",
            "‚úì ÏµúÏ¢Ö Î™®Îç∏ ÌïôÏäµ ÏôÑÎ£å!\n",
            "  ÏµúÍ≥† Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 0.9091 (90.91%)\n",
            "\n",
            "======================================================================\n",
            "16. ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
            "======================================================================\n",
            "‚úì ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞: 500Í∞ú\n",
            "\n",
            "======================================================================\n",
            "17. ÌÖåÏä§Ìä∏ Dataset ÏÉùÏÑ±\n",
            "======================================================================\n",
            "‚úì ÌÖåÏä§Ìä∏ DataLoader ÏÉùÏÑ± ÏôÑÎ£å\n",
            "\n",
            "======================================================================\n",
            "18. ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ•ò\n",
            "======================================================================\n",
            "\n",
            "Î∂ÑÎ•ò Ï§ë...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:07<00:00,  8.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úì Î∂ÑÎ•ò ÏôÑÎ£å!\n",
            "\n",
            "======================================================================\n",
            "19. Í≤∞Í≥º Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ±\n",
            "======================================================================\n",
            "‚úì Í≤∞Í≥º Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ± ÏôÑÎ£å\n",
            "‚úì Ïª¨Îüº: ['idx', 'conversation', 'prediction', 'confidence']\n",
            "\n",
            "======================================================================\n",
            "20. Í≤∞Í≥º Ï†ÄÏû• (Google Drive)\n",
            "======================================================================\n",
            "‚úì ÏòàÏ∏° Í≤∞Í≥º Ï†ÄÏû•: /content/drive/My Drive/predictions.csv\n",
            "‚úì Ï†ÄÏû•Îêú Ìñâ Ïàò: 500Í∞ú\n",
            "\n",
            "======================================================================\n",
            "21. Optuna ÌäúÎãù Î≥¥Í≥†ÏÑú ÏÉùÏÑ±\n",
            "======================================================================\n",
            "‚úì Optuna ÌäúÎãù Î≥¥Í≥†ÏÑú Ï†ÄÏû•: /content/drive/My Drive/optuna_tuning_report.txt\n",
            "\n",
            "\n",
            "\n",
            "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "‚ñà                                                                    ‚ñà\n",
            "‚ñà                   ‚úÖ Optuna ÏûêÎèô ÌäúÎãù Î∞è Î™®Îì† ÏûëÏóÖÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§!                 ‚ñà\n",
            "‚ñà                                                                    ‚ñà\n",
            "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "\n",
            "üìÅ Ï†ÄÏû•Îêú ÌååÏùº Ï†ïÎ≥¥:\n",
            "\n",
            "  üìä ÌååÏùº 1: predictions.csv\n",
            "     ÏúÑÏπò: My Drive/predictions.csv\n",
            "     ÎÇ¥Ïö©: ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ•ò Í≤∞Í≥º\n",
            "     Ìñâ Ïàò: 500Í∞ú\n",
            "\n",
            "  üìã ÌååÏùº 2: optuna_tuning_report.txt\n",
            "     ÏúÑÏπò: My Drive/optuna_tuning_report.txt\n",
            "     ÎÇ¥Ïö©: Optuna ÌäúÎãù ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑú (F1 Score Ìè¨Ìï®)\n",
            "\n",
            "üéØ ÏµúÏ†Å ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ (Optuna ÏÑ†Ï†ï):\n",
            "  ‚Ä¢ Batch Size: 8\n",
            "  ‚Ä¢ Epochs: 4\n",
            "  ‚Ä¢ Learning Rate: 1.25e-05\n",
            "  ‚Ä¢ Max Length: 256\n",
            "\n",
            "üìà ÏµúÏ¢Ö Î™®Îç∏ ÏÑ±Îä•:\n",
            "  ‚Ä¢ Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 0.9091 (90.91%)\n",
            "  ‚Ä¢ Í≤ÄÏ¶ù F1 Score: 0.9084 (90.84%)\n",
            "  ‚Ä¢ ÎÜíÏùÄ Ïã†Î¢∞ÎèÑ ÏòàÏ∏°: 415Í∞ú (83.0%)\n",
            "\n",
            "‚è±Ô∏è  ÏàòÌñâ ÏãúÍ∞Ñ:\n",
            "  ‚Ä¢ Optuna ÌäúÎãù: 2398.9Ï¥à (40.0Î∂Ñ)\n",
            "  ‚Ä¢ Ï¥ù 5Î≤à ÏãúÎèÑ\n",
            "\n",
            "‚ú® F1 Score ÌôúÏö©:\n",
            "  ‚Ä¢ Í≤ÄÏ¶ù Îã®Í≥Ñ: Accuracy Í∏∞Î∞ò ÏµúÏ†ÅÌôî (F1 ScoreÎäî Î™®ÎãàÌÑ∞ÎßÅ)\n",
            "  ‚Ä¢ ÌÖåÏä§Ìä∏ Îã®Í≥Ñ: Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò Î∂ÑÎ•ò + F1 Ï†ïÎ≥¥ Ï†úÍ≥µ\n",
            "  ‚Ä¢ ÌïôÏäµÏóê ÏòÅÌñ• ÏóÜÏùå: ÏµúÏ†ÅÌôî Í∏∞Ï§ÄÏùÄ Accuracy Ïú†ÏßÄ\n",
            "\n",
            "üíæ Google DriveÏóêÏÑú ÌååÏùºÏùÑ Îã§Ïö¥Î°úÎìúÌïòÍ±∞ÎÇò ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§!\n",
            "\n",
            "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ÏïÑÏõÉÌíã ÌååÏùºÏùÑ ÌÜµÌïú Í≤∞Í≥º ÏöîÏïΩ\n",
        "# ============================================================================\n",
        "import pandas as pd\n",
        "\n",
        "df_pred = pd.read_csv('/content/drive/My Drive/predictions.csv')\n",
        "\n",
        "print(\"\\n\" + \"üéØ ÏòàÏ∏° Í≤∞Í≥º ÏöîÏïΩ\".center(50, \"=\"))\n",
        "print(f\"\\nÏ¥ù ÏòàÏ∏°: {len(df_pred):,}Í∞ú\\n\")\n",
        "\n",
        "print(\"üìä ÌÅ¥ÎûòÏä§Î≥Ñ Î∂ÑÌè¨:\")\n",
        "\n",
        "if 'confidence' in df_pred.columns:\n",
        "    print(f\"\\nüìå Ï†ÑÏ≤¥ Ïã†Î¢∞ÎèÑ ÏöîÏïΩ:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"ÌèâÍ∑†: {df_pred['confidence'].mean():.4f}\")\n",
        "    print(f\"ÏµúÏÜå: {df_pred['confidence'].min():.4f}\")\n",
        "    print(f\"ÏµúÎåÄ: {df_pred['confidence'].max():.4f}\")\n",
        "\n",
        "    high = (df_pred['confidence'] >= 0.9).sum()\n",
        "    mid = ((df_pred['confidence'] >= 0.7) & (df_pred['confidence'] < 0.9)).sum()\n",
        "    low = (df_pred['confidence'] < 0.7).sum()\n",
        "\n",
        "    print(f\"\\nÎÜíÏùÄ Ïã†Î¢∞ÎèÑ (‚â•0.9):   {high:>5}Í∞ú ({high/len(df_pred)*100:>5.1f}%)\")\n",
        "    print(f\"Ï§ëÍ∞Ñ Ïã†Î¢∞ÎèÑ (0.7~0.9): {mid:>5}Í∞ú ({mid/len(df_pred)*100:>5.1f}%)\")\n",
        "    print(f\"ÎÇÆÏùÄ Ïã†Î¢∞ÎèÑ (<0.7):   {low:>5}Í∞ú ({low/len(df_pred)*100:>5.1f}%)\")\n",
        "\n",
        "    # ‚ú® ÌÅ¥ÎûòÏä§Î≥Ñ ÏÉÅÏÑ∏ Ïã†Î¢∞ÎèÑ Î∂ÑÌè¨\n",
        "    print(f\"\\nüìå ÌÅ¥ÎûòÏä§Î≥Ñ Ïã†Î¢∞ÎèÑ ÏÉÅÏÑ∏ Î∂ÑÏÑù:\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    # ÌèâÍ∑† Ïã†Î¢∞ÎèÑ Í∏∞Ï§ÄÏúºÎ°ú Ï†ïÎ†¨\n",
        "    class_stats = []\n",
        "    for class_name in df_pred['prediction'].unique():\n",
        "        class_data = df_pred[df_pred['prediction'] == class_name]\n",
        "        avg_conf = class_data['confidence'].mean()\n",
        "        class_stats.append((class_name, avg_conf))\n",
        "\n",
        "    class_stats.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    for class_name, _ in class_stats:\n",
        "        class_data = df_pred[df_pred['prediction'] == class_name]\n",
        "\n",
        "        count = len(class_data)\n",
        "        avg_conf = class_data['confidence'].mean()\n",
        "        min_conf = class_data['confidence'].min()\n",
        "        max_conf = class_data['confidence'].max()\n",
        "        std_conf = class_data['confidence'].std()\n",
        "\n",
        "        high_class = (class_data['confidence'] >= 0.9).sum()\n",
        "        mid_class = ((class_data['confidence'] >= 0.7) & (class_data['confidence'] < 0.9)).sum()\n",
        "        low_class = (class_data['confidence'] < 0.7).sum()\n",
        "\n",
        "        high_pct = high_class / count * 100 if count > 0 else 0\n",
        "        mid_pct = mid_class / count * 100 if count > 0 else 0\n",
        "        low_pct = low_class / count * 100 if count > 0 else 0\n",
        "\n",
        "        print(f\"\\nüîπ {class_name} (Ï¥ù {count}Í∞ú)\")\n",
        "        print(f\"   ÌèâÍ∑†: {avg_conf:.4f} | ÏµúÏÜå: {min_conf:.4f} | ÏµúÎåÄ: {max_conf:.4f} | ÌëúÏ§ÄÌé∏Ï∞®: {std_conf:.4f}\")\n",
        "        print(f\"   ÎÜíÏùå (‚â•0.9):   {high_class:>4}Í∞ú ({high_pct:>5.1f}%)\", end=\"\")\n",
        "        print(f\" {'‚ñà' * int(high_pct / 5)}\")\n",
        "        print(f\"   Ï§ëÍ∞Ñ (0.7~0.9): {mid_class:>4}Í∞ú ({mid_pct:>5.1f}%)\", end=\"\")\n",
        "        print(f\" {'‚ñà' * int(mid_pct / 5)}\")\n",
        "        print(f\"   ÎÇÆÏùå (<0.7):   {low_class:>4}Í∞ú ({low_pct:>5.1f}%)\", end=\"\")\n",
        "        print(f\" {'‚ñà' * int(low_pct / 5)}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyGtiPne12yq",
        "outputId": "c891e766-9409-4cc6-b76a-27e847bd0a28"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================üéØ ÏòàÏ∏° Í≤∞Í≥º ÏöîÏïΩ====================\n",
            "\n",
            "Ï¥ù ÏòàÏ∏°: 500Í∞ú\n",
            "\n",
            "üìä ÌÅ¥ÎûòÏä§Î≥Ñ Î∂ÑÌè¨:\n",
            "\n",
            "üìå Ï†ÑÏ≤¥ Ïã†Î¢∞ÎèÑ ÏöîÏïΩ:\n",
            "--------------------------------------------------\n",
            "ÌèâÍ∑†: 0.9401\n",
            "ÏµúÏÜå: 0.3628\n",
            "ÏµúÎåÄ: 0.9988\n",
            "\n",
            "ÎÜíÏùÄ Ïã†Î¢∞ÎèÑ (‚â•0.9):     415Í∞ú ( 83.0%)\n",
            "Ï§ëÍ∞Ñ Ïã†Î¢∞ÎèÑ (0.7~0.9):    46Í∞ú (  9.2%)\n",
            "ÎÇÆÏùÄ Ïã†Î¢∞ÎèÑ (<0.7):      39Í∞ú (  7.8%)\n",
            "\n",
            "üìå ÌÅ¥ÎûòÏä§Î≥Ñ Ïã†Î¢∞ÎèÑ ÏÉÅÏÑ∏ Î∂ÑÏÑù:\n",
            "==========================================================================================\n",
            "\n",
            "üîπ ÌòëÎ∞ï ÎåÄÌôî (Ï¥ù 121Í∞ú)\n",
            "   ÌèâÍ∑†: 0.9566 | ÏµúÏÜå: 0.4661 | ÏµúÎåÄ: 0.9986 | ÌëúÏ§ÄÌé∏Ï∞®: 0.0995\n",
            "   ÎÜíÏùå (‚â•0.9):    104Í∞ú ( 86.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "   Ï§ëÍ∞Ñ (0.7~0.9):   11Í∞ú (  9.1%) ‚ñà\n",
            "   ÎÇÆÏùå (<0.7):      6Í∞ú (  5.0%) \n",
            "\n",
            "üîπ Í∞àÏ∑® ÎåÄÌôî (Ï¥ù 98Í∞ú)\n",
            "   ÌèâÍ∑†: 0.9464 | ÏµúÏÜå: 0.5270 | ÏµúÎåÄ: 0.9975 | ÌëúÏ§ÄÌé∏Ï∞®: 0.0985\n",
            "   ÎÜíÏùå (‚â•0.9):     84Í∞ú ( 85.7%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "   Ï§ëÍ∞Ñ (0.7~0.9):    8Í∞ú (  8.2%) ‚ñà\n",
            "   ÎÇÆÏùå (<0.7):      6Í∞ú (  6.1%) ‚ñà\n",
            "\n",
            "üîπ ÏßÅÏû• ÎÇ¥ Í¥¥Î°≠Ìûò ÎåÄÌôî (Ï¥ù 122Í∞ú)\n",
            "   ÌèâÍ∑†: 0.9462 | ÏµúÏÜå: 0.4512 | ÏµúÎåÄ: 0.9988 | ÌëúÏ§ÄÌé∏Ï∞®: 0.1398\n",
            "   ÎÜíÏùå (‚â•0.9):    109Í∞ú ( 89.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "   Ï§ëÍ∞Ñ (0.7~0.9):    1Í∞ú (  0.8%) \n",
            "   ÎÇÆÏùå (<0.7):     12Í∞ú (  9.8%) ‚ñà\n",
            "\n",
            "üîπ Í∏∞ÌÉÄ Í¥¥Î°≠Ìûò ÎåÄÌôî (Ï¥ù 145Í∞ú)\n",
            "   ÌèâÍ∑†: 0.9231 | ÏµúÏÜå: 0.3628 | ÏµúÎåÄ: 0.9982 | ÌëúÏ§ÄÌé∏Ï∞®: 0.1354\n",
            "   ÎÜíÏùå (‚â•0.9):    109Í∞ú ( 75.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "   Ï§ëÍ∞Ñ (0.7~0.9):   23Í∞ú ( 15.9%) ‚ñà‚ñà‚ñà\n",
            "   ÎÇÆÏùå (<0.7):     13Í∞ú (  9.0%) ‚ñà\n",
            "\n",
            "üîπ ÏùºÎ∞ò ÎåÄÌôî (Ï¥ù 14Í∞ú)\n",
            "   ÌèâÍ∑†: 0.8772 | ÏµúÏÜå: 0.3885 | ÏµúÎåÄ: 0.9979 | ÌëúÏ§ÄÌé∏Ï∞®: 0.1801\n",
            "   ÎÜíÏùå (‚â•0.9):      9Í∞ú ( 64.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "   Ï§ëÍ∞Ñ (0.7~0.9):    3Í∞ú ( 21.4%) ‚ñà‚ñà‚ñà‚ñà\n",
            "   ÎÇÆÏùå (<0.7):      2Í∞ú ( 14.3%) ‚ñà‚ñà\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOIKRGYCBm-5"
      },
      "source": [
        "ÏöîÏïΩ Î∞è Í∞úÏÑ†Ïïà\n",
        "1) Ïñ¥Ï†ú ÏµúÍ≥† ÌèâÏ†êÏù¥ ÎÇòÏôîÎçò Í∞úÏÑ†ÏïàÏù¥ ÌÜ†ÏöîÏùº ÏÉàÎ≤Ω Î≤ÑÏ†ÑÏùò ÏïÑÏõÉÌíãÏù∏Îç∞ Ïù¥ÌõÑ Í≥ÑÏÜç ÏàòÏ†ïÎêòÍ≥† ÏôîÍ∏∞ ÎïåÎ¨∏Ïóê, ÌòÑÏû¨ Ïñ¥Îñ§ ÏÉÅÌÉúÏóêÏÑú Í∑∏Îü∞ Í≤∞Í≥ºÎ¨ºÏù¥ ÎÇòÏôîÎäîÏßÄ Í∑∏ÎåÄÎ°ú Ïû¨ÌòÑÎêòÍ≥† ÏûàÏßÄ ÏïäÏùå (ÏïàÌÉÄÍπùÏäµÎãàÎã§.) Í∑∏Îü¨ÎÇò, Í∞úÏÑ†Î∞©Ìñ•Í≥º Ìè¨ÌÖêÏÖúÏùÄ Î™ÖÌôïÌïòÍ≤å Î≥¥ÏûÑ\n",
        "1) ÏùºÎ∞ò ÎåÄÌôîÏùò Îã§ÏñëÏÑ± Î∂ÄÏó¨ Î∞è Í∞ïÌôî. Ïù¥Î•º ÏúÑÌï¥ ÏùºÎ∞ò ÎåÄÌôî ÏÉùÏÑ±Í∏∞Î•º ÎßåÎì§Ïñ¥ Î≥º ÏÉùÍ∞ÅÏûÑ. Í∑∏ÎûòÏïº Ïó¨Îü¨Í∞ÄÏßÄ Î°úÏßÅÏùÑ Î∞òÏòÅÌïú ÏùºÎ∞ò ÎåÄÌôîÎ•º ÏÉùÏÑ±ÌïòÍ≥† ÌÜµÌï©Ìï¥Ïïº ÌíçÎ∂ÄÌïú Ï†ïÎ≥¥Î•º Îã¥ÏùÄ ÏùºÎ∞ò ÎåÄÌôî Ìï©ÏÑ±Ïù¥ Í∞ÄÎä•Îê† Í≤ÉÏúºÎ°ú ÏòàÏ∏°Îê®\n",
        "2) Î∂ÑÎ•òÏãú ÌôïÎ•†Ïóê Îî∞Îùº Ïã§ÌñâÌïòÎäîÎç∞, Ïù¥Îïå ÌôïÎ•†Ïóê Îî∞Î•∏ Ïª®ÌîºÎçòÏä§Î•º Í∞ÄÏßÄÍ≥† Î∂ÑÎ•ò Ìï®. Î∂àÍ∑†ÌòïÏ†ÅÏù∏ Î∂ÑÎ•ò ÏÑ±Îä•ÏùÑ Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌïòÏó¨ ÎÇÆÏùÄ Ïª®ÌîºÎçòÏä§Î•º Í∞ÄÏßÑ Í≤ΩÏö∞ ÏùºÎ∞ò ÎåÄÌôîÎ°ú Î∂ÑÎ•òÌïòÎäî Î∞©Ïãù Ï†ÅÏö© (ÌòπÏùÄ  ÏûÑÍ≥ÑÍ∞í Î∞©Ïãù Î∞òÏòÅ)Ìï¥ Î≥¥Í≥†Ïûê Ìï®. Ï∂îÍ∞Ä ÏΩîÎìú Î∏îÎü≠Ïóê ÏòàÏãúÎ•º Ï†úÏãúÌï®"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ï∂îÍ∞Ä Î∏îÎü≠**"
      ],
      "metadata": {
        "id": "dgAbcTod51oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ï†úÏ∂úÌè¨Îß∑ Î≥ÄÍ≤Ω - path Ïûò ÏßÄÏ†ïÌï¥ÏïºÌï©ÎãàÎã§.\n",
        "# 1. CSV ÌååÏùº ÏùΩÍ∏∞\n",
        "df = pd.read_csv('/content/predictions._bertbeoki (1).csv') #Ìå®Ïä§ ÏßÄÏ†ï!\n",
        "\n",
        "print(\"ÏõêÎ≥∏ ÌååÏùº ÌôïÏù∏:\")\n",
        "print(f\"Ìñâ Ïàò: {len(df)}\")\n",
        "print(f\"Ïó¥: {df.columns.tolist()}\")\n",
        "print(\"\\nÏ≤òÏùå 5Ìñâ:\")\n",
        "print(df.head())\n",
        "\n",
        "# 2. idxÏôÄ prediction Ïó¥Îßå ÏÑ†ÌÉù\n",
        "df_selected = df[['idx', 'prediction']].copy()\n",
        "\n",
        "# 3. prediction Ïó¥ÏùÑ classÎ°ú Ïù¥Î¶Ñ Î≥ÄÍ≤Ω\n",
        "df_selected.rename(columns={'prediction': 'class'}, inplace=True)\n",
        "\n",
        "# 4. Îß§Ìïë ÎîïÏÖîÎÑàÎ¶¨ ÏÉùÏÑ±\n",
        "class_mapping = {\n",
        "    'ÌòëÎ∞ï ÎåÄÌôî': 0,\n",
        "    'Í∞àÏ∑® ÎåÄÌôî': 1,\n",
        "    'ÏßÅÏû• ÎÇ¥ Í¥¥Î°≠Ìûò ÎåÄÌôî': 2,\n",
        "    'Í∏∞ÌÉÄ Í¥¥Î°≠Ìûò ÎåÄÌôî': 3,\n",
        "    'ÏùºÎ∞ò ÎåÄÌôî': 4\n",
        "}\n",
        "\n",
        "# 5. prediction Í∞íÏùÑ Ïà´ÏûêÎ°ú Î≥ÄÌôò\n",
        "# Note: The column is now named 'class', so we apply the mapping to that column.\n",
        "df_selected['class'] = df_selected['class'].map(class_mapping)\n",
        "\n",
        "\n",
        "# 6. Í≤∞Í≥º ÌôïÏù∏\n",
        "print(\"\\n\\nÎ≥ÄÌôò ÌõÑ ÌååÏùº ÌôïÏù∏:\")\n",
        "print(f\"Ìñâ Ïàò: {len(df_selected)}\")\n",
        "print(f\"Ïó¥: {df_selected.columns.tolist()}\")\n",
        "print(f\"Îç∞Ïù¥ÌÑ∞ Î™®Ïñë: {df_selected.shape}\") # Add this line to show the shape\n",
        "print(\"\\nÏ≤òÏùå 10Ë°å:\")\n",
        "print(df_selected.head(10))\n",
        "\n",
        "# 7. Î≥ÄÌôòÎêú Í∞í ÌôïÏù∏\n",
        "print(\"\\n\\nÎ≥ÄÌôò ÌÜµÍ≥Ñ:\")\n",
        "print(df_selected['class'].value_counts().sort_index())\n"
      ],
      "metadata": {
        "id": "2QVghl8h3eSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Îç∞Ïù¥ÌÑ∞ ÏûÑÎ≤†Îî© Íµ¨Ï°∞ Î∞è ÌÖåÏä§Ìä∏, ÌÖåÏä§Ìä∏ Î∏îÎü≠\n",
        "# ============================================================================\n",
        "# üîç Î™®Îç∏ Íµ¨Ï°∞ ÏßÑÎã®\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"Î™®Îç∏ Íµ¨Ï°∞ ÏßÑÎã®\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
        "\n",
        "# 1. Config ÌôïÏù∏\n",
        "config = AutoConfig.from_pretrained('beomi/kcbert-base')\n",
        "print(f\"\\nüìã Î™®Îç∏ Config:\")\n",
        "print(f\"  ‚Ä¢ max_position_embeddings: {config.max_position_embeddings}\")\n",
        "print(f\"  ‚Ä¢ hidden_size: {config.hidden_size}\")\n",
        "print(f\"  ‚Ä¢ vocab_size: {config.vocab_size}\")\n",
        "\n",
        "# 2. ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÌôïÏù∏\n",
        "tokenizer = AutoTokenizer.from_pretrained('beomi/kcbert-base')\n",
        "print(f\"\\nüî§ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä:\")\n",
        "print(f\"  ‚Ä¢ model_max_length: {tokenizer.model_max_length}\")\n",
        "print(f\"  ‚Ä¢ vocab_size: {len(tokenizer)}\")\n",
        "\n",
        "# 3. Ïã§Ï†ú Î™®Îç∏ Î°úÎìú ÌõÑ ÏûÑÎ≤†Îî© ÌÅ¨Í∏∞ ÌôïÏù∏\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'beomi/kcbert-base',\n",
        "    num_labels=len(label_encoder.classes_)\n",
        ")\n",
        "\n",
        "print(f\"\\nüß† Î™®Îç∏ Î†àÏù¥Ïñ¥:\")\n",
        "print(f\"  ‚Ä¢ Position Embeddings: {model.bert.embeddings.position_embeddings.weight.shape}\")\n",
        "print(f\"  ‚Ä¢ Token Embeddings: {model.bert.embeddings.word_embeddings.weight.shape}\")\n",
        "print(f\"  ‚Ä¢ Classifier: {model.classifier.weight.shape}\")\n",
        "\n",
        "# 4. ÌÖåÏä§Ìä∏ ÏûÖÎ†•ÏúºÎ°ú ÌôïÏù∏\n",
        "test_text = \"ÌÖåÏä§Ìä∏ Î¨∏Ïû•ÏûÖÎãàÎã§.\"\n",
        "# Use the smaller of tokenizer's model_max_length and config's max_position_embeddings\n",
        "max_seq_length = min(tokenizer.model_max_length, config.max_position_embeddings)\n",
        "\n",
        "test_encoding = tokenizer(test_text, max_length=max_seq_length, padding='max_length',\n",
        "                          truncation=True, return_tensors='pt')\n",
        "print(f\"\\nüß™ ÌÖåÏä§Ìä∏ ÏûÖÎ†•:\")\n",
        "print(f\"  ‚Ä¢ input_ids shape: {test_encoding['input_ids'].shape}\")\n",
        "print(f\"  ‚Ä¢ attention_mask shape: {test_encoding['attention_mask'].shape}\")\n",
        "\n",
        "try:\n",
        "    with torch.no_grad():\n",
        "        output = model(**test_encoding)\n",
        "    print(f\"  ‚Ä¢ Î™®Îç∏ Ï∂úÎ†•: ‚úÖ Ï†ïÏÉÅ\")\n",
        "except Exception as e:\n",
        "    print(f\"  ‚Ä¢ Î™®Îç∏ Ï∂úÎ†•: ‚ùå ÏóêÎü¨ - {str(e)}\")\n",
        "\n",
        "print(\"=\"*70 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdmuQA5oe1l1",
        "outputId": "74edd5a3-63ae-43f4-e1f3-d01635594f42"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Î™®Îç∏ Íµ¨Ï°∞ ÏßÑÎã®\n",
            "======================================================================\n",
            "\n",
            "üìã Î™®Îç∏ Config:\n",
            "  ‚Ä¢ max_position_embeddings: 300\n",
            "  ‚Ä¢ hidden_size: 768\n",
            "  ‚Ä¢ vocab_size: 30000\n",
            "\n",
            "üî§ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä:\n",
            "  ‚Ä¢ model_max_length: 300\n",
            "  ‚Ä¢ vocab_size: 30000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß† Î™®Îç∏ Î†àÏù¥Ïñ¥:\n",
            "  ‚Ä¢ Position Embeddings: torch.Size([300, 768])\n",
            "  ‚Ä¢ Token Embeddings: torch.Size([30000, 768])\n",
            "  ‚Ä¢ Classifier: torch.Size([5, 768])\n",
            "\n",
            "üß™ ÌÖåÏä§Ìä∏ ÏûÖÎ†•:\n",
            "  ‚Ä¢ input_ids shape: torch.Size([1, 300])\n",
            "  ‚Ä¢ attention_mask shape: torch.Size([1, 300])\n",
            "  ‚Ä¢ Î™®Îç∏ Ï∂úÎ†•: ‚úÖ Ï†ïÏÉÅ\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ïª®ÌîºÎçòÏä§ Í∏∞Î∞ò Î∂ÑÎ•ò\n",
        "# ÎÇÆÏùÄ confidenceÎ•º Í∞ÄÏßÑ Í∞àÏ∑® ‚Üí ÏùºÎ∞òÏúºÎ°ú Ïû¨Î∂ÑÎ•ò\n",
        "def reclassify_low_confidence(df, confidence_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Í∞àÏ∑®Î°ú Î∂ÑÎ•òÎêêÏßÄÎßå ÌôïÏã†Ïù¥ ÎÇÆÏúºÎ©¥ ÏùºÎ∞òÏúºÎ°ú Î≥ÄÍ≤Ω\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Í∞àÏ∑®Ïù∏Îç∞ confidenceÍ∞Ä ÎÇÆÏùÄ Í≤ΩÏö∞\n",
        "    mask = (df_copy['prediction'] == 'Í∞àÏ∑® ÎåÄÌôî') & (df_copy['confidence'] < confidence_threshold)\n",
        "    df_copy.loc[mask, 'prediction'] = 'ÏùºÎ∞ò ÎåÄÌôî'\n",
        "\n",
        "    print(f\"Ïû¨Î∂ÑÎ•òÎêú ÏÉòÌîå Ïàò: {mask.sum()}Í∞ú\")\n",
        "    return df_copy\n",
        "\n",
        "# ÏÇ¨Ïö©\n",
        "df_improved = reclassify_low_confidence(df, confidence_threshold=0.75)"
      ],
      "metadata": {
        "id": "lZtYwY_BOUjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXhs2x8fBqPr"
      },
      "outputs": [],
      "source": [
        "# ÏûÑÍ≥ÑÍ∞í Î∂ÄÏó¨ Î∞©Ïãù Î∂ÑÎ•ò\n",
        "# ÌòÑÏû¨: Í∞ÄÏû• ÎÜíÏùÄ ÌôïÎ•†Ïùò ÌÅ¥ÎûòÏä§Î•º ÏÑ†ÌÉù\n",
        "predicted_class = np.argmax(predictions)\n",
        "\n",
        "# Í∞úÏÑ†: ÏûÑÍ≥ÑÍ∞í Ï°∞Ï†ï\n",
        "def predict_with_threshold(predictions, thresholds):\n",
        "    \"\"\"\n",
        "    thresholds: {0: 0.6, 1: 0.5, 2: 0.5, 3: 0.5, 4: 0.3}\n",
        "    Í∞àÏ∑®(1)Îäî ÎÜíÏùÄ ÌôïÎ•† ÌïÑÏöî, ÏùºÎ∞ò(4)ÏùÄ ÎÇÆÏùÄ ÌôïÎ•†Î°úÎèÑ ÌóàÏö©\n",
        "    \"\"\"\n",
        "    max_class = np.argmax(predictions)\n",
        "    max_prob = predictions[max_class]\n",
        "\n",
        "    # ÏùºÎ∞ò ÎåÄÌôî Ïö∞ÏÑ† Í≤ÄÌÜ†\n",
        "    if predictions[4] > thresholds[4]:  # ÏùºÎ∞òÏù¥ 30% Ïù¥ÏÉÅÏù¥Î©¥\n",
        "        return 4\n",
        "\n",
        "    # Í∞àÏ∑®Îäî ÏóÑÍ≤©ÌïòÍ≤å\n",
        "    if max_class == 1 and max_prob < thresholds[1]:  # Í∞àÏ∑®Í∞Ä 50% ÎØ∏ÎßåÏù¥Î©¥\n",
        "        return 4  # ÏùºÎ∞òÏúºÎ°ú Î∂ÑÎ•ò\n",
        "\n",
        "    return max_class\n",
        "\n",
        "# ÏÇ¨Ïö© ÏòàÏãú\n",
        "thresholds = {\n",
        "    0: 0.6,  # ÌòëÎ∞ï\n",
        "    1: 0.7,  # Í∞àÏ∑® (ÏóÑÍ≤©ÌïòÍ≤å!)\n",
        "    2: 0.6,  # ÏßÅÏû• ÎÇ¥ Í¥¥Î°≠Ìûò\n",
        "    3: 0.6,  # Í∏∞ÌÉÄ Í¥¥Î°≠Ìûò\n",
        "    4: 0.2   # ÏùºÎ∞ò (Í¥ÄÎåÄÌïòÍ≤å!)\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}