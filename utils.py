import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns # Add seaborn import
import tempfile
import shutil

import re
import random
from typing import List, Optional, Union
import pandas as pd
from tqdm import tqdm

# matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic' # Windows
plt.rcParams['axes.unicode_minus'] = False # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€

from preprocessing import load_and_preprocess_data
from tokenization import train_sentencepiece_model, SentencePieceVocab


def hist_conversations_length(train_path, test_path, vocab_size=1320):
    """
    í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ ê° ë ˆì´ë¸”ë³„ í† í°í™” í›„ conversationsì˜ length ë¶„í¬ë¥¼
    ì‹œê°í™”í•˜ê³ , ì‹œê°í™” ì´ë¯¸ì§€ë¥¼ Images ë””ë ‰í† ë¦¬ì— ì €ì¥

    Args:
        train_path: train data csv path
        test_path: test data csv path
        vocab_size
    """
    print("ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì¤‘...")
    train_conversations, train_labels, _, _, class_to_idx = \
        load_and_preprocess_data(train_path, test_path)

    # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    temp_dir = tempfile.mkdtemp()
    model_prefix = os.path.join(temp_dir, 'temp_spm_for_hist')
    
    print("SentencePiece ëª¨ë¸ í•™ìŠµ ì¤‘ (í•™ìŠµ ë°ì´í„° ê¸°ì¤€)...")
    try:
        # í•™ìŠµ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ SentencePiece ëª¨ë¸ í•™ìŠµ
        sp_model_path = train_sentencepiece_model(
            train_conversations, model_prefix=model_prefix, vocab_size=vocab_size
        )
        vocab = SentencePieceVocab(sp_model_path)

        print("í† í°í™” ë° ê¸¸ì´ ê³„ì‚° ì¤‘...")
        # í•™ìŠµ ë°ì´í„°ì˜ ê¸¸ì´ ê³„ì‚° ([CLS], [EOS] í¬í•¨)
        # [CLS], [EOS] í† í°ì€ encodeí•¨ìˆ˜ì—ì„œ ìƒì„±í•˜ì§€ ì•Šê³  Dataset í´ë˜ìŠ¤ì—ì„œ ì¶”ê°€í•˜ë¯€ë¡œ +2 í•´ì•¼í•¨
        all_train_lengths = [len(vocab.encode(conv)) + 2 for conv in train_conversations]

        # ë ˆì´ë¸”ë³„ ê¸¸ì´ ë¶„í¬ ì‹œê°í™”
        idx_to_class = {v: k for k, v in class_to_idx.items()}
        unique_labels = sorted(list(set(train_labels)))
        num_labels = len(unique_labels)

        # ì„œë¸Œí”Œë¡¯ ê·¸ë¦¬ë“œ í¬ê¸° ì¡°ì • (ì˜ˆ: 2x3 ë˜ëŠ” 3x2)
        nrows = (num_labels + 1) // 2 if num_labels > 1 else 1
        ncols = 2 if num_labels > 0 else 1
        if num_labels == 0: # Handle case with no labels
            print("No labels found in training data to plot.")
            return

        fig, axes = plt.subplots(nrows, ncols, figsize=(15, 5 * nrows))
        fig.suptitle('Distribution of Conversation Lengths by Label (Train Data)', fontsize=16)
        
        # axesê°€ 1ì°¨ì› ë°°ì—´ì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ í‰íƒ„í™”
        if nrows * ncols == 1:
            axes = [axes]
        else:
            axes = axes.flatten()

        for i, label_idx in enumerate(unique_labels):
            ax = axes[i]
            label_name = idx_to_class[label_idx]
            
            # í•´ë‹¹ ë ˆì´ë¸”ì— ì†í•˜ëŠ” ëŒ€í™” ê¸¸ì´ í•„í„°ë§
            label_lengths = [all_train_lengths[j] for j, lbl in enumerate(train_labels) if lbl == label_idx]

            if not label_lengths:
                ax.set_title(f'{label_name} (No data)')
                ax.set_xlabel('Length of Conversation (tokens)')
                ax.set_ylabel('Frequency')
                continue

            sns.histplot(label_lengths, bins=50, kde=True, ax=ax, color=sns.color_palette("tab10")[i % 10])
            
            mean_len = np.mean(label_lengths)
            median_len = np.median(label_lengths)
            percentile_95 = np.percentile(label_lengths, 95)

            ax.axvline(mean_len, color='red', linestyle='dashed', linewidth=2, label=f'Mean: {mean_len:.2f}')
            ax.axvline(median_len, color='green', linestyle='dashed', linewidth=2, label=f'Median: {median_len:.2f}')
            ax.axvline(percentile_95, color='purple', linestyle='dotted', linewidth=2, label=f'95th: {percentile_95:.2f}')

            ax.set_title(f'{label_name} (count: {len(label_lengths)})')
            ax.set_xlabel('Length of Conversation (tokens)')
            ax.set_ylabel('Frequency')
            ax.legend()
            ax.grid(True)
        
        # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì„œë¸Œí”Œë¡¯ ìˆ¨ê¸°ê¸°
        for j in range(i + 1, len(axes)):
            fig.delaxes(axes[j])

        plt.tight_layout() # ì „ì²´ íƒ€ì´í‹€ì„ ìœ„í•œ ì—¬ë°± í™•ë³´
        plt.show()

        # --- ê·¸ë˜í”„ ì´ë¯¸ì§€ ì €ì¥ --- #
        images_dir = './Images'
        if not os.path.exists(images_dir):
            os.makedirs(images_dir)
        
        save_path = os.path.join(images_dir, 'conversation_length_distributions_by_label.png')
        fig.savefig(save_path)
        print(f"ê·¸ë˜í”„ê°€ {save_path} ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    finally:
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ë° íŒŒì¼ ì‚­ì œ
        print(f"ì„ì‹œ ë””ë ‰í† ë¦¬({temp_dir}) ë° íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.")
        shutil.rmtree(temp_dir)


class TextAugmenter:
    """í…ìŠ¤íŠ¸ ë°ì´í„° ì¦ê°• í´ë˜ìŠ¤"""
    
    def __init__(self, dropout_rate=0.15, exclude_labels=None):
        """
        Args:
            dropout_rate: ë‹¨ì–´ ì‚­ì œ ë¹„ìœ¨
            exclude_labels: ì¦ê°•í•˜ì§€ ì•Šì„ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [4] for ì¼ë°˜ëŒ€í™”)
        """
        self.dropout_rate = dropout_rate
        self.exclude_labels = set(exclude_labels) if exclude_labels else set()
    
    def apply_word_dropout(self, text):
        """ëœë¤ ë‹¨ì–´ ì‚­ì œ"""
        if pd.isna(text) or not isinstance(text, str):
            return text
        
        words = text.split()
        if len(words) <= 2:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ì¦ê°•í•˜ì§€ ì•ŠìŒ
            return text
        
        new_words = []
        for word in words:
            if random.random() > self.dropout_rate:
                new_words.append(word)
        
        # ìµœì†Œ 1ê°œ ë‹¨ì–´ëŠ” ìœ ì§€
        return ' '.join(new_words) if new_words else words[0]
    
    def augment_row(self, row, text_columns):
        """ë°ì´í„° í–‰ ì¦ê°•"""
        augmented_row = row.copy()
        for col in text_columns:
            if col in augmented_row:
                augmented_row[col] = self.apply_word_dropout(augmented_row[col])
        return augmented_row


def augment_csv(
    input_csv_path,
    output_csv_path,
    text_columns,
    label_column='label',
    augment_ratio=2,
    dropout_rate=0.15,
    exclude_labels=None
):
    """
    CSV íŒŒì¼ ë°ì´í„° ì¦ê°•
    
    Args:
        input_csv_path: ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ
        output_csv_path: ì¶œë ¥ CSV íŒŒì¼ ê²½ë¡œ
        text_columns: ì¦ê°•í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['input_text', 'target_text'])
        label_column: ë¼ë²¨ ì»¬ëŸ¼ëª… (ê¸°ë³¸ê°’: 'label')
        augment_ratio: ì¦ê°• ë°°ìˆ˜ (2 = ì›ë³¸ + 2ë°° ì¦ê°• = 3ë°° ë°ì´í„°)
        dropout_rate: ë‹¨ì–´ ì‚­ì œ ë¹„ìœ¨
        exclude_labels: ì¦ê°•í•˜ì§€ ì•Šì„ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì¦ê°•ëœ ë°ì´í„°í”„ë ˆì„
    """
    print(f"ğŸ“‚ Reading CSV: {input_csv_path}")
    df = pd.read_csv(input_csv_path)
    
    print(f"ğŸ“Š Original data size: {len(df)}")
    print(f"ğŸ“‹ Columns: {df.columns.tolist()}")
    
    # ë¼ë²¨ë³„ í†µê³„
    if label_column in df.columns:
        print(f"\nğŸ“ˆ Label distribution:")
        print(df[label_column].value_counts().sort_index())
    
    # Augmenter ìƒì„±
    augmenter = TextAugmenter(dropout_rate=dropout_rate, exclude_labels=exclude_labels)
    exclude_labels_set = set(exclude_labels) if exclude_labels else set()
    
    # ì¦ê°•ëœ ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸
    augmented_data = []
    
    # ì›ë³¸ ë°ì´í„° ì¶”ê°€
    augmented_data.append(df)
    
    # ë¼ë²¨ë³„ë¡œ ì¦ê°•
    if label_column in df.columns:
        for label in df[label_column].unique():
            # ì œì™¸ ë¼ë²¨ì€ ì¦ê°•í•˜ì§€ ì•ŠìŒ
            if label in exclude_labels_set:
                print(f"\nâ­ï¸  Skipping label {label} (excluded)")
                continue
            
            label_df = df[df[label_column] == label]
            print(f"\nğŸ”„ Augmenting label {label}: {len(label_df)} samples Ã— {augment_ratio}")
            
            # augment_ratioë§Œí¼ ì¦ê°•
            for i in range(augment_ratio):
                augmented_rows = []
                for _, row in tqdm(label_df.iterrows(), 
                                  total=len(label_df), 
                                  desc=f"  Round {i+1}/{augment_ratio}"):
                    augmented_row = augmenter.augment_row(row, text_columns)
                    augmented_rows.append(augmented_row)
                
                augmented_data.append(pd.DataFrame(augmented_rows))
    else:
        # ë¼ë²¨ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ì „ì²´ ë°ì´í„° ì¦ê°•
        print(f"\nğŸ”„ Augmenting all data Ã— {augment_ratio}")
        for i in range(augment_ratio):
            augmented_rows = []
            for _, row in tqdm(df.iterrows(), 
                              total=len(df), 
                              desc=f"  Round {i+1}/{augment_ratio}"):
                augmented_row = augmenter.augment_row(row, text_columns)
                augmented_rows.append(augmented_row)
            
            augmented_data.append(pd.DataFrame(augmented_rows))
    
    # ëª¨ë“  ë°ì´í„° í•©ì¹˜ê¸°
    final_df = pd.concat(augmented_data, ignore_index=True)
    
    # ì…”í”Œ
    final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    print(f"\nâœ… Final augmented data size: {len(final_df)}")
    
    if label_column in final_df.columns:
        print(f"\nğŸ“ˆ Final label distribution:")
        print(final_df[label_column].value_counts().sort_index())
    
    # CSV ì €ì¥
    print(f"\nğŸ’¾ Saving to: {output_csv_path}")
    final_df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')
    
    print("âœ… Done!")
    return final_df